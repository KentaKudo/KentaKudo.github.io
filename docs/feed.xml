<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Blog — Kenta Kudo</title>
        <link>http://localhost:3000/feed.xml</link>
        <description>What I write about when I write about technology</description>
        <lastBuildDate>Tue, 25 Jan 2022 23:43:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/nuxt-community/feed-module</generator>
        <item>
            <title><![CDATA[Does integer division overflow?]]></title>
            <link>http://localhost:3000/blog/division-overflow</link>
            <guid>http://localhost:3000/blog/division-overflow</guid>
            <description><![CDATA[In this short post, I'm going to introduce integer overflow and small fun fact that causes overflow in seemingly impossible situation.]]></description>
            <content:encoded><![CDATA[
## What's integer overflow?

> In computer programming, an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of digits – either higher than the maximum or lower than the minimum representable value. — [Wikipedia — Integer overflow](https://en.wikipedia.org/wiki/Integer_overflow)

As you may know, addition of two integer numbers can overflow. For example addition of two `int8` number 127 and 1 overflows, and doesn't return mathematically correct answer.

```go
package main

import "fmt"

func main() {
        var i, j int8 = 127, 1
        fmt.Printf("127 + 1 = %d", i+j)
}
```

This Go programme outputs `127 + 1 = -128`. For subtraction, if you think about `127 - (-1)`, this is going to overflow as this is the equivalent expression to the addition we just saw.

Likewise, multiplication of two integer numbers can overflow.

```go
package main

import "fmt"

func main() {
        var i, j int8 = 64, 2
        fmt.Printf("64 * 2 = %d", i*j)
}
```

This outputs `64 * 2 = -128`.

The reason is because 127 is the biggest number `int8` can represent and any calculation resulting in bigger number than this overflows returning unexpected result.

Here is a question: Can division of two integer numbers ever overflow? If yes, which combination of two integers make it happen?

## Integer division overflow

From mathematical point of view, if the divisor is either bigger than or equal to 1, or smaller than or equal to -1, the absolute value of the quotient is never going to be bigger than that of the dividend, and for any `int8` number this condition is satisfied. So it doesn't seem possible to make overflow happen with two `int8` numbers.

…However, yes it is possible, as you may have guessed.

What do you think is the output of this Go programme?

```go
package main

import "fmt"

func main() {
        var i, j int8 = -128, -1
        fmt.Printf("-128 / -1 = %d", i/j)
}
```

Mathematically correct answer is `128`…but wait, **wasn't the biggest number `int8` can represent 127??**

That means this calculation overflows and the output is `-128 / -1 = -128`.

The reason why this happens is while `int8` can represent `2^8 = 256` different numbers, 0 has to be included, which means odd number `255` is left to represent both positive and negative numbers. Most of the computing systems adopt a technique called _two's complement_ to represent signed numbers[^1] and with that technique `int8` is going to have **127 positive numbers and 128 negative numbers**.

As a result by dividing the biggest negative number by -1, we can cause overflow in integer division.

I'd like to know more of this kind of "fun facts", so please share if you know. In the next post I'd like to talk about what two's complement is, and why it is so widely used in the computing systems.

[^1]: Signed numbers: Numbers that consist of both positive and negative numbers
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Power of NAND]]></title>
            <link>http://localhost:3000/blog/the-power-of-nand</link>
            <guid>http://localhost:3000/blog/the-power-of-nand</guid>
            <description><![CDATA[When I was an engineering student, I learned a logic gate NAND in one of the electronics classes. At that time I didn't fully understand why NAND is so important.

Ten years later, only after I decided to engage myself more in computer science, I finally understood why it's so: We can build very complex computing systems only with NAND on contrary to its super simple look.

I'd like to introduce here (1) what NAND is and (2) why it has such a powerful capability.]]></description>
            <content:encoded><![CDATA[
## What's NAND?

NAND is one of the most basic **logic gates**. So first I need to explain what the logic gate is.

A logic gate is a sort of a calculator that accepts one or more input and produces one or more output where both input and output values are either 0 or 1. Let's take a look at one of the simplest logic gates: AND.

<figure>
    <img src="/img/blog/the-power-of-nand/AND.svg" alt="AND gate" />
    <figcaption>AND gate</figcaption>
</figure>

As you can see it takes two inputs (a) and (b), then produce one output (out). Because each (a) and (b) can be either 0 or 1, there are 4 variations of input combination as shown in the table below. The output of an AND gate will be 1 only when both (a) and (b) are 1.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  1  |  0  |  0  |
|  0  |  1  |  0  |
|  1  |  1  |  1  |

<aside>This table representation of the relationship between input combinations and corresponding output is called <i>truth table</i>.</aside>

Let's take a look at another example: OR gate.

<figure>
    <img src="/img/blog/the-power-of-nand/OR.svg" alt="OR gate" />
    <figcaption>OR gate</figcaption>
</figure>

Again, it receives two inputs and produces one output. The output of the OR gate is 0 when both (a) and (b) are 0, otherwise 1. This is the truth table of the OR gate.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  1  |  0  |  1  |
|  0  |  1  |  1  |
|  1  |  1  |  1  |

Now let's get back to the initial question: What is NAND?

Like AND and OR gates, NAND is also a logic gate that receives two inputs and produces one output.

<figure>
    <img src="/img/blog/the-power-of-nand/NAND.svg" alt="NAND gate" />
    <figcaption>NAND gate</figcaption>
</figure>

The output of NAND gate is just an opposite of AND output: When either (a) or (b) is 1 or both of them are 0, the output is 1. More simply put, the output is 0 only when both (a) and (b) are 1.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  1  |
|  1  |  0  |  1  |
|  0  |  1  |  1  |
|  1  |  1  |  0  |

Not very complicated, right?

Now how would it be possible to build a "computer" like the one you're reading this post with with this tiny device?

## How to Build a Computer with NAND

The steps are like this:

1. build elementary logic gates such as AND, OR, and Multiplexer with NAND
1. build components needed to compose CPU such as ALU (Arithmetic Logic Unit) and RAM (Random Access Memories) with elementary logic gates
1. build a CPU with these components

An essential fact that makes these steps possible is that _any logic gates can be constructed by combining only NAND gates_. Let's take a look at an example.

<figure>
    <img src="/img/blog/the-power-of-nand/AND with NAND.svg" alt="Input (a) and (b) are connected to two different NANDs. The output of these two NANDs are connected to another NAND." />
    <figcaption>???</figcaption>
</figure>

There are three NAND gates in this combined logic gate and it still has two inputs and one output. The branching lines from (a) and (b) mean that input (a) and (b) are connected to multiple gates. Please take a moment to think how the truth table of this logic gate would look like.

This is the answer.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  1  |  0  |  0  |
|  0  |  1  |  0  |
|  1  |  1  |  1  |

As you can see, it's identical to the AND gate; we successfully made an AND gate only with NAND gates.

What about this one?

<figure>
    <img src="/img/blog/the-power-of-nand/OR with NAND.svg" alt="Input (a) are branched into two and connected to the same NAND. Same connection for (b). The output of these two NANDs are connected to another NAND." />
    <figcaption>???</figcaption>
</figure>

As you may have guessed, it's an OR gate implemented with NAND gates.

The truth tables we've seen so far were quite simple: two inputs and one output, but it can be very complex in real life. However, it is known that no matter how complex the truth table is, it can be realised by combining only NAND gates. This characteristic of NAND is called **functional completeness**.

<aside>In fact NAND is not the only gate that has functional completeness: NOR gate (the inverse of OR gate), or the combination of AND and NOT also has this property.</aside>

Because so called CPU and any other components needed to build a computer are just a very complex version of logic gates, all the necessary components to build a computer are constructed only with NANDs.

Of course tons of arduous work needs to be done on top of this to turn it into a full-fledged computer we use today, however, they are all about how to make use of this tiny yet powerful device.

## "The Elements of Computing Systems"

Let me introduce a book which I learned this fact from at the end of the story.

In the previous post I said I'm going to work on the "CPU Experiment". However, I didn't know what exactly I should work on or how to kick off the project. After some intensive search on the internet, I came across this book titled ["The Elements of Computing Systems"](https://www.amazon.co.uk/Elements-Computing-Systems-second-Principles/dp/0262539802/).

The book was written by Dr. Noam Nisan and Dr. Shimon Schocken, and was first published in 2005 as a computer science textbook to teach their students how the black-boxed computer systems actually look inside.

The key concept of the book is "NAND to Tetris": The idea that any general-purpose computers are made in a same way that

- (1) they are built from NAND as an elementary logic gate, and
- (2) they are programmed to run any applications such as Tetris game.

And then there is a word _to_ between them meaning the book provides readers a hands-on experience of building a Tetris (general-purpose computer) starting from NAND (an elementary logic gate).

It turned out that not only is this book a perfect introduction to the low-level systems but also it was a perfect timing for me to read. Most importantly, it helped me understand what exactly I'm trying to learn.

I can't wait to see what comes out of this journey. Stay tuned…
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CPU Experiment at the University of Tokyo]]></title>
            <link>http://localhost:3000/blog/cpu-experiment</link>
            <guid>http://localhost:3000/blog/cpu-experiment</guid>
            <description><![CDATA["CPU Experiment" at the University of Tokyo (UT) CS undergrad course is one of the most famous (either in a good way or bad way) programmes among those who are concerned.

Here, I want to introduce what the programme is, and what kind of exciting stuff are going on there.]]></description>
            <content:encoded><![CDATA[
I note in passing that this is NOT based on my personal experience because I was not a CS student at UT. So apologies if I'm not very accurate in some details but I hope I don't miss the big picture.

## What's CPU Experiment

The programme is held in the autumn-winter term in the third year. CS undergrads are divided into a group of 3–5 people, and they'll be given a credit if they can successfully run the ray-tracing app written in MinCaml (a subset of OCaml), which is handed out at the introduction, at the end term presentation.

The rules and conditions have changed over time but basically what they need to do are:

- define ISA (original or based on PowerPC, MIPS, RISC-V, etc.)
- build a CPU core on a provided FPGA ([KCU105](https://www.xilinx.com/products/boards-and-kits/kcu105.html))
- write an assembler
- extend a [MinCaml compiler](https://github.com/esumii/min-caml) so it can compile the given ray-tracing app
- write a simulator to debug the compiler/assembler
- compile & run the required ray-tracing app on the self-built CPU

Each team member is assigned to one of the four tasks: CPU (core), FPU, simulator, and compiler. Conventionally the assembler seems to be done by a member who is responsible for the simulator.

What's cool about this on-the-job style programme I think is not only you can learn how to integrate the computer science knowledge you learned through the course, but also you need to finish the project in time collaborating with other team members.

## That's not the end of the story

This is where things get even more exciting.

For most of them it's not very difficult to run the required ray-tracing app, and actually they succeed in running the app in less than 3months.

How do they spend the rest of the term?

It's up to them how to use the time: Most of the teams try to optimise the system to amaze people by how fast they can run the app at the end term presentation.

The optimisation ideas include:

- design richer ISA
- introduce pipelining
- optimise compiler to reduce the number of instructions
- and so on

The current record seems to be 5s, which was made in 2017(\*), and generally speaking under 20s is considered to be brazing fast.

(\* The team realised multi-core CPU & the compiler that utilises it.)

However, some teams decide to venture into the wild adventure…

## Creative (crazy) ideas they try

As long as the ray-tracing app works, you can be credited for the course. There's nothing to prevent them from tackling their creative ideas such as:

- run an OS such as xv6, Linux, or even write an original OS
- run the ray-tracing app on the OS
- implement a single instruction that runs the ray-tracing
- etc.

There are lots of things involved to achieve these: To run an OS you'll need much richer ISA than just running the ray-tracing app. If you want to run the existing OS written in C on your original ISA, you'll need a C compiler that targets the ISA …and so on.

Some of them were realised by their exhaustive effort, while the others are yet to come true. So exciting, aren't they?

## Why am I writing this?

Some may wonder why I'm writing a post to introduce something I didn't do. Well, there are two reasons:

First of all, I think the programme is very cool and worth sharing. There are lots of blog posts written by students but most of them are in Japanese (see the next section), and only few resources are available in English for their cool stuff.

And the second one, rather personal one, is that I'm interested in tracing this "CPU Experiment" as my personal project. I've been working as a software engineer for almost 7 years. Thankfully I could grow a lot in this space, yet I've been feeling I missed chances to learn computer science basics. I thought it's a great idea to follow what they do to fill this gap.

I hope I can give updates on my project soon:)

## References

Even though most of them are written in Japanese, I'll leave some links for those who are interested so you can at least have a look at their code:)

2010

- [CPU 実験](https://wata-orz.hatenadiary.org/entry/20100319/1268998396)

2015

- [CPU 実験で自作 CPU に UNIX ライク OS (xv6) を移植した話](https://nullpo-head.hateblo.jp/entry/2015/03/24/205419)
- [EN][how we ran a unix-like os (xv6) on our home-built cpu with our home-built c compiler](https://fuel.edby.coffee/posts/how-we-ported-xv6-os-to-a-home-built-cpu-with-a-home-built-c-compiler/)
  - This is basically the same article as the one above but in English.
- [自作 CPU 向け C コンパイラをつくって OS 動かした話 (CPU 実験まとめ)](https://kw-udon.hatenablog.com/entry/2015/03/19/171921)

2016

- [東大理情名物の CPU 実験で毎週徹夜したお話（概要編）](https://medium.com/eureka-engineering/%E6%9D%B1%E5%A4%A7%E7%90%86%E6%83%85%E5%90%8D%E7%89%A9%E3%81%AEcpu%E5%AE%9F%E9%A8%93%E3%81%A7%E6%AF%8E%E9%80%B1%E5%BE%B9%E5%A4%9C%E3%81%97%E3%81%9F%E3%81%8A%E8%A9%B1-%E6%A6%82%E8%A6%81%E7%B7%A8-6cb8c155cb64)

2017

- [CPU 実験：マルチコアで並列実行するまで（コンパイラ係目線）](https://eguchishi.hatenablog.com/entry/2017/09/09/150229)
- [CPU 実験 2016 年度 D 班コア係（CPU 実験でマルチコア）](https://sueki743.hatenablog.jp/entry/2017/03/30/151822)

2018

- [東大の CPU 実験で自作コア上の自作 OS 上で自作シェルを動かした話](https://travelingresearcher.com/entry/2018/02/27/172417)

2019

- [RISC V に従う CPU の上で動作する OS を Rust で書く（CPU 実験余興）](https://moraprogramming.hateblo.jp/entry/2019/03/17/165802)

2020

- [Linux が動作する RISC-V CPU を自作した (2019 年度 CPU 実験 余興)](https://diary.shift-js.info/building-a-riscv-cpu-for-linux/)
- [CPU 実験が終わりました 〜コア係目線〜](https://misteer.hatenablog.com/entry/cpuex)
- [CPU 実験の振り返り (コンパイラ係目線)](https://okuraofvegetable.hatenablog.com/entry/2020/02/29/230201)

2021

- [CPU 実験振り返り(シミュレータ・FPU 係目線)](https://note.com/tomo_stleq/n/ncc8a1ff8ba20)
- [ああ CPU 実験](https://cfkazu.hatenablog.com/entry/2020/12/05/000416)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NFT — 可能性と問題点]]></title>
            <link>http://localhost:3000/blog/nft-potential-and-problems</link>
            <guid>http://localhost:3000/blog/nft-potential-and-problems</guid>
            <description><![CDATA[NFTの話題が連日テックニュース界隈を賑わせています。きっかけはデジタルアート作品のNFTに$69mの値段がついたことです。作品は Beepleというアーティストによるもので、取引はNFTを扱うオークションサイトのChristie'sで行われました。その後もTwitterの創業者であるJack Dorseyによる、Twitter初のツイートに$3mの値段がつくなど、話題がつきません。
NFTはデジタル作品を手がけるアーティストを支援する強力なツールとなる可能性がある一方でいくつかの問題も抱えています。ここではNFTのもつ可能性と問題点についてまとめます。]]></description>
            <content:encoded><![CDATA[
まずはじめに NFT とはなにか簡単に説明します。

## NFT とは

NFT とは Non-Fungible Token の略で、ブロックチェーンを応用した技術のひとつです。日本語では非代替性トークンと訳されますが、従来のブロックチェーン技術との違いはまさにその非代替性にあります。

ブロックチェーンは基本的にはデータベースです。ビットコインのような仮想通貨を例にとると、そのブロックチェーン上には誰と誰との間でいくらの移動があったかという取引情報が保存されています。現実世界の通貨においてあなたが持つ 100 円玉と僕が持つ 100 円玉の区別をつける必要がないのと同様に、ビットコインであってもそれぞれのアセットの区別をつける必要はありません。これを代替可能(=fungible)と呼びます。

その一方で世の中には代替不可能なものも存在します。一例としてアート作品を想像してみてください。モナリザのコピーは世の中に多数存在しますが、オリジナルはルーブル美術館にある一つだけです。あなたの持つモナリザのコピーをルーブル美術館のオリジナルと交換することはできません。これを代替不可能(=non-fungible)であると言います。NFT を使ったブロックチェーン上にはそれぞれのアセットを表すユニークな ID とその所有者のマッピングが保存されており、そのブロックチェーンを見ればある作品を所有しているのが誰なのか知ることができます。また、仮想通貨と同様にその所有権を移動させることも可能です。

アート作品がしばしばオークションを通して売りに出されるのと同様に、NFT でもオークションを通して所有権が売買されます。それこそが今 NFT 界隈で起こっていることです。ただし違いはそこで売買されているのがモナリザのような絵画ではなく、画像や映像といったデジタルデータだということです。

## なぜ NFT が盛り上がっているか

デジタルなデータが物理的に存在するモノと圧倒的に異なる点は、オリジナルをコストゼロで簡単にコピーできることです。cmd+c を押すだけで本物と全く同じものを無数に作り出すことができます。それゆえ、これまでデジタルデータは実物のように「所有する」ということができませんでした。

そこに出現したのが NFT です。

さきほど説明したように、NFT を使うことによりブロックチェーン上に"誰が何を所有するか"の情報を書き込むことができ、一度書き込まれた内容を勝手に書き換えることはほぼ不可能です。NFT によりデジタルデータを実物のアート作品と同様に、コレクションとしてあるいは投資対象として「所有する」ということが可能になった、というのが NFT がこれだけ盛り上がっている大きな理由です。

このことは NFT がデジタル作品を手がけるアーティストを支援する強力なツールとなる可能性を示しています。これまで著作権という保護はありながらも、無断でコピーされたものから生み出された利益がその作成者に還元されることはありませんでした。残念ながら NFT はその問題自体に解決策を与えてくれるものではありません。しかし「NFT を通して所有権を売る」という、アーティストが収入を得る新たな手段を生み出しました。加えて多くの NFT プラットフォームでは売買のたびに出品者に手数料が入る仕組みを導入しており、売買が行われる限り継続的に収入を得ることができるという実物の取引にはないメリットもあります。

ここまでをみるとデジタルな創作物に正しい評価が与えられるすばらしい発明、と考えられます。しかし実態は少し異なっているようです。

## 「アーティストがお金を稼ぐ手段」が増えてハッピー？

一見すばらしい仕組みにみえる NFT ですが、問題もいくつか抱えています。

### 1. 誰でも出品できるという欠陥

先述のとおり一度ブロックチェーンに書き込まれた内容は変更することができません。そのため正しい情報を書き込むことが大切です。デジタルデータを扱う NFT の場合は「クリエイター＝出品者」であることが求められますが、これを保証してくれるはずの認証システムの信頼性にはプラットフォームによってばらつきがあります。Rarible という NFT プラットフォーム上では、アーティストがある日起きたら自分の作品が勝手に出品され、売買さえも完了していたという事例[3]が実際に発生しています。また別の NFT プラットフォームである OpenSea は「爆発的に増加した出品リクエストに対応する」と言う名目で認証システムを一切無くしました[3]。あるいは盗作や二次創作物などの出品を防ぐ有効な手段もありません。

クリエイターに新たな収入源を与える一方で、違法者やモラルを無視した人にも等しくその利便性が与えられては仕方がありません。

[3]: https://www.theverge.com/2021/3/20/22334527/nft-scams-artists-opensea-rarible-marble-cards-fraud-art

### 2. 「所有権」というものの曖昧さ

NFT における「所有権」とは実際に何を指すのでしょうか。ブロックチェーン上の各ブロックにはそれがどういった作品なのかという情報が書き込まれなければいけません。しかしブロックチェーンに書き込むことができるデータのサイズには限りがあり、作品をそのままデータとして記録することができません。そこで多くのプラットフォームでは作品への URL(正確には作品の URL を含むメタデータへのリンク)だけをブロック上に記録します。この作品へのリンクをもって所有権というものが成立しています。

しかし URL を使うことには大きな問題があります。もしそのリンクをもつサービスが停止してしまったらどうなるでしょうか。ブロックチェーンは残りますがそこに書き込まれたリンクは無効となり、もうアクセスすることはできません。何を所有しているかわからない「所有権」だけが残ることになります。所有権を URL を使って表現することには信頼性という点で疑問が残ります。

いくつかのプラットフォームではこの問題に対応するため、ブロックに IPFS のトークンを書き込む方式を採用しています。IPFS とは Inter-Planetary File System の略で、ファイルを分散ホスト上に展開し、peer-to-peer システムによりアクセス可能にする技術です。IPFS 上のファイルは URL のような場所を特定する情報は持たず、唯一性をもつトークンのみが与えられます。そしてそのファイルをホストするサーバーが一つでも存在する限りファイルが失われることはありません。

しかしこの IPFS にも問題があります。作品のメタデータは分散して保存されており失われることがありませんが、肝心の作品データが特定のサーバー上にしか存在しないということがあります。URL を使ったケースと同様に、そのサーバーを運用するサービスが停止した時点でそのデータにアクセスすることはできなくなります。

NFT のコンセプトの発案者 Anil Dash はこのブロックチェーンにリンクだけを書き込む方式を"shortcut"としています[4]。その"shortcut"はコンセプトの発案から 7 年たった今も使われ続けています。 NFT における「所有権」とは現状、いつなくなるかわからないとても脆弱なものです。

[4]: https://www.theatlantic.com/ideas/archive/2021/04/nfts-werent-supposed-end-like/618488/

### 3. 環境負荷という問題

ブロックチェーンに新たなブロックを書き込む際の環境負荷という点も見逃せません。多くの NFT プラットフォームではデジタルデータを Ethereum という、すでにあるブロックチェーン上に書き込んでいます。この Ethereum というブロックチェーンは、データの改ざんを防ぐためにプルーフオブワークという仕組みを採用しています。このプルーフオブワークにはマイニングという作業が必要になるのですが、このマイニングにはとてつもない量の計算が必要となります。大量の計算にはそれに見合うだけの電力を必要とします。Ethereum が年間に消費する電力量はベラルーシが年間に消費する電力量に匹敵するといわれています[5]。

これは Ethereum というブロックチェーンが抱える問題であるため、NFT 自身の問題とはいえません。しかし NFT が注目を浴びることで利用者が増え、Ethereum でのマイニング作業がより必要となることで結果的に環境に与える負荷が高まるのは間違いありません。事実今年に入ってから Ethereum の電力消費量は激増しています[5]。

[5]: https://digiconomist.net/ethereum-energy-consumption

## NFT が向かう先

「NFT を通したアート作品が\$69m で落札！」 といったセンセーショナルな話題に振り回されていますが、NFT は本来アーティストを保護し、支援するための仕組みだと Anil Dash は述べています。NFT という技術が大きな可能性を秘めているのは間違いありませんが、出現してから日が浅く、未だ多くの課題を抱えているのも事実です。一過性の話題として消費されるにとどまらず、技術が円熟し有益なものとして利用可能になるのを期待しています。

---

## 参照

- [Beeple sold an NFT for \$69 million](https://www.theverge.com/2021/3/11/22325054/beeple-christies-nft-sale-cost-everydays-69-million)
- [Jack Dorsey’s first tweet sold as an NFT for an oddly specific \$2,915,835.47](https://www.theverge.com/2021/3/22/22344937/jack-dorsey-nft-sold-first-tweet-ethereum-cryptocurrency-twitter)
- [THE CLIMATE CONTROVERSY SWIRLING AROUND NFTS](https://www.theverge.com/2021/3/15/22328203/nft-cryptoart-ethereum-blockchain-climate-change)
- [NFT MANIA IS HERE, AND SO ARE THE SCAMMERS](https://www.theverge.com/2021/3/20/22334527/nft-scams-artists-opensea-rarible-marble-cards-fraud-art)
- [YOUR MILLION-DOLLAR NFT CAN BREAK TOMORROW IF YOU’RE NOT CAREFUL](https://www.theverge.com/2021/3/25/22349242/nft-metadata-explained-art-crypto-urls-links-ipfs)
- [Do You Really\* Own Your NFT? Chances Are, You Don’t](https://thedefiant.io/do-you-really-own-your-nft-chances-are-you-dont/)
- [NFTs Weren’t Supposed to End Like This](https://www.theatlantic.com/ideas/archive/2021/04/nfts-werent-supposed-end-like/618488/)
- [The Non-Fungible Token Bible: Everything you need to know about NFTs](https://opensea.io/blog/guides/non-fungible-tokens)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[量子コンピュータ開発の現状]]></title>
            <link>http://localhost:3000/blog/quantum-computer-2021</link>
            <guid>http://localhost:3000/blog/quantum-computer-2021</guid>
            <description><![CDATA[2019 年の 10 月に Google が世界で初めて量子超越性を達成したと発表して以来、量子コンピュータ開発競争はますます盛り上がりをみせています。量子コンピュータの実現を目指す各社ともに順調に開発を進めており、この先数年のあいだに量子超越性の証明が相次いで発表されることが予想されます。しかしその先の万能量子コンピュータの実現にはエラー耐性という課題を克服しなければならず、それには 10 年単位の研究・開発が必要となるでしょう。
ここでは2021年3月現在における各社の量子コンピュータ開発への取り組みをそれぞれ見ていきたいと思います。]]></description>
            <content:encoded><![CDATA[
## 量子コンピュータとは

本題の前に簡単に量子コンピュータについて。

量子コンピュータとはその名の通り量子力学を応用した原理で動作するコンピュータのことです。現在ひろく使われているコンピュータに対し指数関数的な性能向上が理論上は実現可能であることから注目を集めています。

量子コンピュータの計算には*量子重ね合わせ*という量子力学の性質を利用します。従来のコンピュータ(これより古典コンピュータと呼びます)は*ビット*を基本単位として計算を行いますが、このビットは 0 または 1 の値しかとらず、全ての計算はこの 0 と 1 の配列をもとに行われます。これに対し、量子コンピュータは*量子ビット*(qubit; quantum bit)を基本単位としています。この量子ビットに特殊な操作を加えることにより、0 と 1 が重なり合った—0 であると同時に 1 でもある—状態にすることができます。この状態の量子を使って計算を行うことにより、古典コンピュータでは 2 の n 乗回必要だった計算をたった一回で行うことが可能になります。

もうひとつ、量子コンピュータの特徴的な性質として計算結果が確率的であることがあげられます。一回の計算で得られた解が必ずしも正しいという保証はなく、何度か計算を繰り返して解の精度を高める必要があります。複数回の試行には時間がかかるとはいえ、定数倍の計算量が必要になるだけなのでそれでもやはり古典コンピュータよりも量子コンピュータの方が高速に動作します。

## 量子コンピュータ開発の歴史

1980 年代に Paul Benioff や Richard Fyneman がその有効性を予測して以来、実現に向けた研究が続けられています。

数ある研究の中でもっとも注目を集めたのが 1994 年に Peter Shor が考案した「ショアのアルゴリズム」です。内容としては、量子コンピュータにより素因数分解を高速に解くアルゴリズムを発見した、というシンプルなものですがとても重要な意味を持っています。素因数分解は数が大きくなればなるほど計算に時間がかかるようになります。十分に大きな数の素因数分解を古典コンピュータ(スーパーコンピュータであったとしても)で解こうとすると何万年という時間がかかるため、現実的には計算不可能です。この性質を利用したのが RSA 暗号という、現在インターネットでもっとも広く使われている暗号化技術のひとつです。ショアのアルゴリズムは**量子コンピュータが実現した際には RSA 暗号が破られる**ということを示しており、この発見により量子コンピュータ研究はさらに注目を集めるようになります。

その後はしばらく基礎研究で進展をみせるものの、実用化という面では大きな変化がなく 2010 年代を迎えます。そして 2011 年 5 月、突如カナダの D-Wave Systems という企業が「世界初の商用量子コンピュータを開発した」と発表し世間を驚かせます。その後 IBM や Google などのハイテク企業が商用利用のための開発に参加し、量子コンピュータを実際にビジネスに利用するための開発競争が加速します。

2019 年の 10 月には、Google が世界で初めて量子超越性—量子力学を利用した計算機で古典コンピュータでは解決不可能な計算を行うこと—を達成したと発表[1]。これは量子コンピュータ研究におけるひとつのマイルストーンとみられています。

[1]: https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html

## 量子コンピューティングに取り組む企業

現在商用の量子コンピュータを開発、またはクラウドサービスとして提供している主な企業を紹介します。

### D-Wave Systems

先ほども登場しました D-Wave Systems は 2011 年 5 月、世界初の商用量子コンピュータとして D-Wave One を発表[2]しました。

現在量子コンピュータの実装方式として主流なのは量子ゲートと呼ばれるものですが、D-Wave が採用しているのは量子アニーリングと呼ばれる方式です。この方式は量子ゲート方式に比べると解決可能な問題の種類は制限される一方、量子の状態を管理するのが容易というメリットがあります。量子ゲート方式が現在 50~100 程度の量子ビットしか扱えないのに対し、D-Wave は 2000qubit を達成しています。

先月には量子優位性—量子コンピュータが古典コンピュータの性能を上回ること、量子超越性よりも弱い概念—を発表[3]しており、量子コンピュータの実用においては一歩抜け出しています。

D-Wave はクラウドコンピューティングサービスである Leap2 を提供しているほか、のちほど紹介する Amazon Braket からも利用可能です。

[2]: https://www.nature.com/articles/nature10012
[3]: https://www.zdnet.com/article/a-quantum-computer-just-solved-a-decades-old-problem-three-million-times-faster-than-a-classical-computer/

### Google

先にも述べましたが、Google は 2019 年の 10 月に「量子超越性を達成した」と発表しました。Google はこの証明のため Sycamore という量子ゲート方式のプロセサを開発、Summit—当時世界でもっともパワフルなスーパーコンピュータ—が 10,000 年かけて行う計算を 200 秒で完了したという実験結果を示しました。

この Sycamore プロセサは NISQ(Noisy Intermediate-Scale Quantum)というアイディアにもとづいて開発されています。この NISQ というのは理論物理学者の John Preskill が 2018 年の論文[4]で発表したもので、
(a) Noisy、つまりエラー耐性を実装していないことと、
(b) 中規模なサイズ(50~100qubit)
な点が特徴です。Google はこの NISQ という考え方をベースに Sycamore プロセサを開発し、53qubit での計算を行いました。

Sycamore プロセサは Google のクラウドサービスである Quantum Computing Service で利用可能となっていますが、一般にはまだ公開されていません。また Google は量子コンピュータ利用を支援するツールとして Cirq SDK や OpenFermion といった OSS を公開しています。

[4]: https://arxiv.org/abs/1801.00862

### Honeywell

電子機器関連の老舗 Honeywell はイオントラップ型と呼ばれる方式を使った量子コンピュータを開発しています。

イオントラップはその名のとおり、電磁場を使ってイオンを閉じ込める方式です。このイオントラップ型は
(a) 他の方式に比べて計算の精度が高いことと、
(b) コンピュータを大規模化するためのアイデアが構築されていること
から有力視されている方式のひとつです。

量子コンピュータの性能を測る指標のひとつに Quantum Volume というものがあります。これは IBM が提唱した指標で、量子コンピュータの動作方式によらず計算できるように定義されています。この Quantum Volume で高い値をだすためには量子ビット数が多いことももちろんですが、エラー耐性が高いということも大事な要因です。これまでの最高値は昨年 Honeywell が出した 128 という値でしたが、Honeywell は今月この数値を更新、512 という値を得たことを発表[5]しました。Honeywell のシステムは他社に比べ量子ビット数は小さいため、量子ビット数を増やすことよりもエラー耐性を向上させることに力を入れていることが伺えます。

Honeywell のハードウェアは Microsoft Azure から利用可能となっています。

[5]: https://www.zdnet.com/article/quantum-computing-honeywell-just-quadrupled-the-power-of-its-computer/

### IonQ

IonQ は Honeywell と同様にイオントラップ型の量子コンピュータを開発しています。先日 SPAC スキームを使って上場することが発表[6]されましたが、これは量子コンピューティング関連企業としては初となります。

昨年の 10 月にそれまでの 11 量子ビットシステムからほぼ 3 倍となる 32 量子ビットのシステムを発表[7]しました。その発表の中には Quantum Volume で 4,000,000 を達成したという驚くべき内容も含まれています。Honeywell や IBM とは異なる計算の仕方をしているので単純に比較することはできませんが、計算精度が高いという彼らの主張を裏付ける数値と言えそうです。

また、IonQ が昨年発表したロードマップ[8]によると 2023 年には量子コンピュータのデータセンターを作ることも予定しています。

IonQ のハードウェアは Amazon Braket や Microsoft Azure のクラウドサービスから利用可能なほか、直接パートナーシップを結ぶことで利用が可能です。

[6]: https://ionq.com/news/march-08-2021-ionq-to-become-first-public-quantum-computer-company/
[7]: https://ionq.com/news/october-01-2020-most-powerful-quantum-computer
[8]: https://ionq.com/posts/december-09-2020-scaling-quantum-computer-roadmap

### IBM Quantum

IBM は 2016 年に超伝導型量子コンピュータを利用したクラウドサービスである IBM Quantum を発表しています。

彼らは Python をベースにした SDK である Qiskit フレームワークを公開しているのに加え、Qiskit をベースにしたビジュアルプログラミングプラットフォームの Quantum Composer というサービスを提供しています。インタラクティブなツールにより、より直感的に量子コンピュータの操作を学ぶことができます。

IBM の量子コンピュータ開発に関するニュースの中で注目したいのは昨年の 9 月に発表したロードマップ[9]です。そのロードマップによると IBM は今年には 127qubit のシステムの実現を予定しています。Google の Sycamore が 53qubit で量子超越性を達成したことを考慮すると、同じく NISQ を採用する IBM がどこまで性能をあげてくるか注目です。また 2023 年には 1,000qubit を達成すると計画しています。

[9]: https://www.ibm.com/blogs/research/2020/09/ibm-quantum-roadmap/

### Amazon

Amazon は 2019 年の 12 月にクラウド量子コンピューティングサービス、Amazon Braket をリリースしています。Amazon 自身が量子コンピュータを開発するのではなく、量子コンピュータを開発する他社とパートナーシップを結びそのリソースを AWS 上で提供する形をとっています。Rigetti(超伝導型)、IonQ(イオントラップ型)、D-Wave(量子アニーリング型)とそれぞれ異なる方式を採用する会社と提携してサービスを提供しているのが特徴です。昨年の 8 月には一般公開をしており、誰でも利用することが可能です。また、シミュレータ上でプログラムを実行することもできます。

サポートツールとして Amazon が提供する Amazon Braket SDK や D-Wave が開発する Ocean SDK の他、PennyLane ライブラリをサポートしています。この PennyLane は Xanadu 社が開発する SDK で、量子コンピュータを利用した機械学習(量子機械学習)を記述するためのものです。

料金体系は 1 ショットあたりの値段と 1 タスクあたりの値段からなります。ショットは 1 回の求解を表す単位です。先に述べたように量子コンピュータの計算は確率的なため、同じ計算を何ショットも繰り返し解の精度を高める必要があります。そうして何千、何万ショット繰り返して解を得る操作を 1 単位としてタスクと呼びます。一例として D-Wave で 1 タスク、2000 ショットの計算をおこなった場合、D-Wave の 1 タスクあたりの料金は$0.30、1ショットあたりの料金は$0.00019 となっているため、1 task \* $0.30 + 2,000 shots \* $0.00019 = \$0.68 となります。

### Azure

Microsoft も Amazon と同様にクラウドプラットフォームである Azure 上で量子コンピューティングサービス Azure Quantum を提供しています。利用可能なハードウェアは Honeywell、IonQ、Quantum Circuits から選択可能です。発表以来 Early Access のステータスでしたが、先月一般公開となり誰でもアクセスすることができます。

Microsoft は Q#言語とその SDK を開発、提供しています。SDK にはシミュレータも含まれているためローカルマシンでプログラムを実行することも可能です。

料金は IonQ のシミュレータで 1 時間あたり\$3,000 となっています。

また、Microsoft は独自の量子コンピュータを製造する研究開発も進めていますが、こちらは少し難航している模様です。2018 年の 3 月にデルフト大学のカウウェンホーフェンがマヨラナ粒子—その存在を予想したイタリアの物理学者にちなんで名付けられた—の存在を確認したという論文を発表します。Microsoft はこのマヨラナ粒子を利用した量子コンピュータの開発を進めてきましたが、現在この 2018 年の論文に疑問が投げかけられており、もしマヨラナ粒子が存在しないということになれば量子コンピュータ開発の計画が大きく後退することは避けられないでしょう。

## おわりに

昨年末には中国が独自に開発する光子を利用した量子コンピュータで量子超越性を達成したことを発表[10]しました。量子超越性とその先にある万能量子コンピュータの実現に向け、各社・各国開発競争を進める中でどうエラー耐性を獲得していくのか。今後も量子コンピュータ関連の話題から目が離せません。

[10]: https://science.sciencemag.org/content/370/6523/1460
]]></content:encoded>
        </item>
    </channel>
</rss>