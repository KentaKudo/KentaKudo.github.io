<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Blog — Kenta Kudo</title>
        <link>http://localhost:3000/feed.xml</link>
        <description>What I write about when I write about technology</description>
        <lastBuildDate>Tue, 17 Oct 2023 12:42:20 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/nuxt-community/feed-module</generator>
        <item>
            <title><![CDATA[DDIA 読書ノート 【第8章】]]></title>
            <link>http://localhost:3000/blog/ddia-chapter-8</link>
            <guid>http://localhost:3000/blog/ddia-chapter-8</guid>
            <description><![CDATA[第8章は分散ノード環境において、認識し、立ち向かわないといけない困難について。分散ノードの敵は誰か、という話。
そしてそれがどういった理由で発生するのかおよびそれを解決することがいかに困難であるかについての理解を深め、最後にそれにどう立ち向かえば良いかの方針が示されている。]]></description>
            <content:encoded><![CDATA[
<aside>
<p><a href="https://open.spotify.com/show/0J8LZwfrGB9BJLihy4Ldb1">London Tech Talk</a> というポッドキャストを運営している <a href="https://twitter.com/tommyasai">@tommyasai</a> さん・<a href="https://twitter.com/kenwagatsuma">@kenwagatsuma</a> さん主催で DDIA の輪読会が行われており、それに向けて DDIA を読んでいるのでその際のまとめです。今回は第 8 章。</p>
<p>理解したこと、考えたことをそのまま書き落としていくので読み物としては読みづらいかと思いますがその点は悪しからず。<p>
</aside>

## 分散システムにおける困難

分散システム特有の困難としては二つ、不確定性と部分障害がある。そしてこれらが発生する理由としても二つ、ネットワークの不確かさとクロックの不確かさを起因としている。

これらを解決するのは非常に困難、あるいは経済性とのトレードオフ上諦めざるを得ないかのどちらかで、特殊な機能要件が求められる環境、例えば金融や航空宇宙システムなどを除いて基本的にはこれらが存在するものとして扱わなければならない。

## 不確かなネットワーク

ネットワークがいかに信頼性に欠けるものであるか、という点は普段異常系の設計をする際にお馴染みかと思う。
ここで知っておく必要があるのは

- フォールトを検知する唯一確実な方法はタイムアウトしかない、ということ
- そしてそのタイムアウトをどのように決めるかの絶対的な方法は存在しない、ということ

の二つである。

どこでフォールトが起こっているかを調べる方法をいくつか例に挙げており、どれも有効な手段ではあるが、究極的にはアプリケーションからの応答がないと処理が実行されたかどうか・成功したかどうかの判定はできないため、確実な方法としてはタイムアウトを待つ、という選択肢だけが残る。

そしてタイムアウトが発生する原因として処理の遅延や輻輳、キューイングの詰まりなどがあるが、いずれの場合も遅延がどのくらいになるかという上限は保証されておらず、レスポンスタイムの分布をもとに経験則的に設定する以外に方法はない。

<aside>
実際には同期ネットワークというものを使うことでネットワークの遅延には上限を設けることが可能ではあるが、その方式はバースト性のあるトラフィックには非常に非効率なため、一般的な通信でその方式が選択されることはない。
</aside>

このようにネットワークに関する性質はとにかく都合の悪い要素が多いのだ。そしてそれはクロックの話題にも当てはまる。

## 不確かなクロック

クロックには時刻を計測するものとある時点からの増加分を数えていく二つの方法があり、単調増加のクロックの方が正確な時刻との同期を取る必要がない分信頼性が高いと言えるが、とはいえ正確に時刻を刻んでいく保証はない。

クロックに依存したくなる典型的な例としてイベントの順序づけがある。

ローカルなクロックに依存してしまうと悲惨で、あるノードは他のノードに対して未来にいるためいろんな悪いことが起こり得る。

これを解決するために、イベントの相対的な発生順序だけを意味する、論理クロックというものを導入できるが、この論理クロックを生成するコンポーネントがボトルネックになりがちである。

<aside>
<p>Google Cloud Spannerはこの問題に対応するため、TrueTime APIを通して、クロックの値の信頼区間を公開している。二つ時刻の信頼区間が重ならないようにすることで、時刻のクロックに依存しながら順序を担保している。</p>
<p>この信頼区間をできるだけ狭くするために、クロックを同期させることに最新の注意を払っている。</p>
</aside>

クロックが信頼できないもう一つの話題としてプロセスの停止について述べている。ガベージコレクションが典型例だ。

車のエアバッグなど、プロセスの停止が致命的な事故につながる可能性のある環境ではプロセスの停止を予測可能なものにする仕組みがあることにはあるが、これもまたトレードオフの話で、OS から何から全てのソフトウェアで一貫してサポートされている必要があるため、その利用は組み込みなどの環境に限られる。

## この困難にどう立ち向かうのか

章の最後にはこの困難に立ち向かう方法としてサイエンスな視点、つまりある点を仮定し、その仮定のもとである性質を満たし続けるアルゴリズムを導入する、という考え方を紹介する。

章の冒頭に述べているように、その仮定が成り立たなくなる事象というのはいつか発生する、発生するものとして捉えないといけないのだが、その境界線を認識することで、具体的にどういうケースでその性質が満たされず、そしてどのように対応するのかということを事前に決めておける、非常に有用な考え方だ。

---

最後に、プログラミングとは全く関係のない感想だが、分散システムと人と人とのコミュニケーションは非常に似ているなと感じた。

相手が何を考えているかを把握する絶対的な手段は存在せず、コミュニケーションに頼らざるを得ない一方で、コミュニケーションはとても不確実性だ。

こうした不確かさを抱えつつも、全体としてはなんとか協調していく必要がある、というのは分散的なものの宿命なのかもしれない。

## References

- [システム障害なしにうるう秒を乗り切る技術の発達について](https://note.com/ruiu/n/n0349ed9f0d8f)
  - クロックに依存する危険性と leap smearing について触れたエッセイ。読み物として面白い。
- [最近よく聞く Quorum は過半数(多数決)よりも一般的でパワフルな概念だった](https://qiita.com/everpeace/items/632831371da5ff215995)
  - Quoram とは何かについて解説した記事。単純に多数決と言えない奥深さを知れる。
- [Predictable low
  latency](https://forum.equinix.com/assets/images/files/Cinnober_on_GC_pause_free_Java_applications.pdf)
  - Java システムの GC のレイテンシに対応するため、ノードを primary-secondary 構成にして、先にレスポンスを返した方の結果を採用する仕組み。どれだけ効果があるのかは不明だが、興味深い。
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[量子コンピュータ開発の現状]]></title>
            <link>http://localhost:3000/blog/quantum-computer-2021</link>
            <guid>http://localhost:3000/blog/quantum-computer-2021</guid>
            <description><![CDATA[2019 年の 10 月に Google が世界で初めて量子超越性を達成したと発表して以来、量子コンピュータ開発競争はますます盛り上がりをみせています。量子コンピュータの実現を目指す各社ともに順調に開発を進めており、この先数年のあいだに量子超越性の証明が相次いで発表されることが予想されます。しかしその先の万能量子コンピュータの実現にはエラー耐性という課題を克服しなければならず、それには 10 年単位の研究・開発が必要となるでしょう。
ここでは2021年3月現在における各社の量子コンピュータ開発への取り組みをそれぞれ見ていきたいと思います。]]></description>
            <content:encoded><![CDATA[
## 量子コンピュータとは

本題の前に簡単に量子コンピュータについて。

量子コンピュータとはその名の通り量子力学を応用した原理で動作するコンピュータのことです。現在ひろく使われているコンピュータに対し指数関数的な性能向上が理論上は実現可能であることから注目を集めています。

量子コンピュータの計算には*量子重ね合わせ*という量子力学の性質を利用します。従来のコンピュータ(これより古典コンピュータと呼びます)は*ビット*を基本単位として計算を行いますが、このビットは 0 または 1 の値しかとらず、全ての計算はこの 0 と 1 の配列をもとに行われます。これに対し、量子コンピュータは*量子ビット*(qubit; quantum bit)を基本単位としています。この量子ビットに特殊な操作を加えることにより、0 と 1 が重なり合った—0 であると同時に 1 でもある—状態にすることができます。この状態の量子を使って計算を行うことにより、古典コンピュータでは 2 の n 乗回必要だった計算をたった一回で行うことが可能になります。

もうひとつ、量子コンピュータの特徴的な性質として計算結果が確率的であることがあげられます。一回の計算で得られた解が必ずしも正しいという保証はなく、何度か計算を繰り返して解の精度を高める必要があります。複数回の試行には時間がかかるとはいえ、定数倍の計算量が必要になるだけなのでそれでもやはり古典コンピュータよりも量子コンピュータの方が高速に動作します。

## 量子コンピュータ開発の歴史

1980 年代に Paul Benioff や Richard Fyneman がその有効性を予測して以来、実現に向けた研究が続けられています。

数ある研究の中でもっとも注目を集めたのが 1994 年に Peter Shor が考案した「ショアのアルゴリズム」です。内容としては、量子コンピュータにより素因数分解を高速に解くアルゴリズムを発見した、というシンプルなものですがとても重要な意味を持っています。素因数分解は数が大きくなればなるほど計算に時間がかかるようになります。十分に大きな数の素因数分解を古典コンピュータ(スーパーコンピュータであったとしても)で解こうとすると何万年という時間がかかるため、現実的には計算不可能です。この性質を利用したのが RSA 暗号という、現在インターネットでもっとも広く使われている暗号化技術のひとつです。ショアのアルゴリズムは**量子コンピュータが実現した際には RSA 暗号が破られる**ということを示しており、この発見により量子コンピュータ研究はさらに注目を集めるようになります。

その後はしばらく基礎研究で進展をみせるものの、実用化という面では大きな変化がなく 2010 年代を迎えます。そして 2011 年 5 月、突如カナダの D-Wave Systems という企業が「世界初の商用量子コンピュータを開発した」と発表し世間を驚かせます。その後 IBM や Google などのハイテク企業が商用利用のための開発に参加し、量子コンピュータを実際にビジネスに利用するための開発競争が加速します。

2019 年の 10 月には、Google が世界で初めて量子超越性—量子力学を利用した計算機で古典コンピュータでは解決不可能な計算を行うこと—を達成したと発表[1]。これは量子コンピュータ研究におけるひとつのマイルストーンとみられています。

[1]: https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html

## 量子コンピューティングに取り組む企業

現在商用の量子コンピュータを開発、またはクラウドサービスとして提供している主な企業を紹介します。

### D-Wave Systems

先ほども登場しました D-Wave Systems は 2011 年 5 月、世界初の商用量子コンピュータとして D-Wave One を発表[2]しました。

現在量子コンピュータの実装方式として主流なのは量子ゲートと呼ばれるものですが、D-Wave が採用しているのは量子アニーリングと呼ばれる方式です。この方式は量子ゲート方式に比べると解決可能な問題の種類は制限される一方、量子の状態を管理するのが容易というメリットがあります。量子ゲート方式が現在 50~100 程度の量子ビットしか扱えないのに対し、D-Wave は 2000qubit を達成しています。

先月には量子優位性—量子コンピュータが古典コンピュータの性能を上回ること、量子超越性よりも弱い概念—を発表[3]しており、量子コンピュータの実用においては一歩抜け出しています。

D-Wave はクラウドコンピューティングサービスである Leap2 を提供しているほか、のちほど紹介する Amazon Braket からも利用可能です。

[2]: https://www.nature.com/articles/nature10012
[3]: https://www.zdnet.com/article/a-quantum-computer-just-solved-a-decades-old-problem-three-million-times-faster-than-a-classical-computer/

### Google

先にも述べましたが、Google は 2019 年の 10 月に「量子超越性を達成した」と発表しました。Google はこの証明のため Sycamore という量子ゲート方式のプロセサを開発、Summit—当時世界でもっともパワフルなスーパーコンピュータ—が 10,000 年かけて行う計算を 200 秒で完了したという実験結果を示しました。

この Sycamore プロセサは NISQ(Noisy Intermediate-Scale Quantum)というアイディアにもとづいて開発されています。この NISQ というのは理論物理学者の John Preskill が 2018 年の論文[4]で発表したもので、
(a) Noisy、つまりエラー耐性を実装していないことと、
(b) 中規模なサイズ(50~100qubit)
な点が特徴です。Google はこの NISQ という考え方をベースに Sycamore プロセサを開発し、53qubit での計算を行いました。

Sycamore プロセサは Google のクラウドサービスである Quantum Computing Service で利用可能となっていますが、一般にはまだ公開されていません。また Google は量子コンピュータ利用を支援するツールとして Cirq SDK や OpenFermion といった OSS を公開しています。

[4]: https://arxiv.org/abs/1801.00862

### Honeywell

電子機器関連の老舗 Honeywell はイオントラップ型と呼ばれる方式を使った量子コンピュータを開発しています。

イオントラップはその名のとおり、電磁場を使ってイオンを閉じ込める方式です。このイオントラップ型は
(a) 他の方式に比べて計算の精度が高いことと、
(b) コンピュータを大規模化するためのアイデアが構築されていること
から有力視されている方式のひとつです。

量子コンピュータの性能を測る指標のひとつに Quantum Volume というものがあります。これは IBM が提唱した指標で、量子コンピュータの動作方式によらず計算できるように定義されています。この Quantum Volume で高い値をだすためには量子ビット数が多いことももちろんですが、エラー耐性が高いということも大事な要因です。これまでの最高値は昨年 Honeywell が出した 128 という値でしたが、Honeywell は今月この数値を更新、512 という値を得たことを発表[5]しました。Honeywell のシステムは他社に比べ量子ビット数は小さいため、量子ビット数を増やすことよりもエラー耐性を向上させることに力を入れていることが伺えます。

Honeywell のハードウェアは Microsoft Azure から利用可能となっています。

[5]: https://www.zdnet.com/article/quantum-computing-honeywell-just-quadrupled-the-power-of-its-computer/

### IonQ

IonQ は Honeywell と同様にイオントラップ型の量子コンピュータを開発しています。先日 SPAC スキームを使って上場することが発表[6]されましたが、これは量子コンピューティング関連企業としては初となります。

昨年の 10 月にそれまでの 11 量子ビットシステムからほぼ 3 倍となる 32 量子ビットのシステムを発表[7]しました。その発表の中には Quantum Volume で 4,000,000 を達成したという驚くべき内容も含まれています。Honeywell や IBM とは異なる計算の仕方をしているので単純に比較することはできませんが、計算精度が高いという彼らの主張を裏付ける数値と言えそうです。

また、IonQ が昨年発表したロードマップ[8]によると 2023 年には量子コンピュータのデータセンターを作ることも予定しています。

IonQ のハードウェアは Amazon Braket や Microsoft Azure のクラウドサービスから利用可能なほか、直接パートナーシップを結ぶことで利用が可能です。

[6]: https://ionq.com/news/march-08-2021-ionq-to-become-first-public-quantum-computer-company/
[7]: https://ionq.com/news/october-01-2020-most-powerful-quantum-computer
[8]: https://ionq.com/posts/december-09-2020-scaling-quantum-computer-roadmap

### IBM Quantum

IBM は 2016 年に超伝導型量子コンピュータを利用したクラウドサービスである IBM Quantum を発表しています。

彼らは Python をベースにした SDK である Qiskit フレームワークを公開しているのに加え、Qiskit をベースにしたビジュアルプログラミングプラットフォームの Quantum Composer というサービスを提供しています。インタラクティブなツールにより、より直感的に量子コンピュータの操作を学ぶことができます。

IBM の量子コンピュータ開発に関するニュースの中で注目したいのは昨年の 9 月に発表したロードマップ[9]です。そのロードマップによると IBM は今年には 127qubit のシステムの実現を予定しています。Google の Sycamore が 53qubit で量子超越性を達成したことを考慮すると、同じく NISQ を採用する IBM がどこまで性能をあげてくるか注目です。また 2023 年には 1,000qubit を達成すると計画しています。

[9]: https://www.ibm.com/blogs/research/2020/09/ibm-quantum-roadmap/

### Amazon

Amazon は 2019 年の 12 月にクラウド量子コンピューティングサービス、Amazon Braket をリリースしています。Amazon 自身が量子コンピュータを開発するのではなく、量子コンピュータを開発する他社とパートナーシップを結びそのリソースを AWS 上で提供する形をとっています。Rigetti(超伝導型)、IonQ(イオントラップ型)、D-Wave(量子アニーリング型)とそれぞれ異なる方式を採用する会社と提携してサービスを提供しているのが特徴です。昨年の 8 月には一般公開をしており、誰でも利用することが可能です。また、シミュレータ上でプログラムを実行することもできます。

サポートツールとして Amazon が提供する Amazon Braket SDK や D-Wave が開発する Ocean SDK の他、PennyLane ライブラリをサポートしています。この PennyLane は Xanadu 社が開発する SDK で、量子コンピュータを利用した機械学習(量子機械学習)を記述するためのものです。

料金体系は 1 ショットあたりの値段と 1 タスクあたりの値段からなります。ショットは 1 回の求解を表す単位です。先に述べたように量子コンピュータの計算は確率的なため、同じ計算を何ショットも繰り返し解の精度を高める必要があります。そうして何千、何万ショット繰り返して解を得る操作を 1 単位としてタスクと呼びます。一例として D-Wave で 1 タスク、2000 ショットの計算をおこなった場合、D-Wave の 1 タスクあたりの料金は$0.30、1ショットあたりの料金は$0.00019 となっているため、1 task \* $0.30 + 2,000 shots \* $0.00019 = \$0.68 となります。

### Azure

Microsoft も Amazon と同様にクラウドプラットフォームである Azure 上で量子コンピューティングサービス Azure Quantum を提供しています。利用可能なハードウェアは Honeywell、IonQ、Quantum Circuits から選択可能です。発表以来 Early Access のステータスでしたが、先月一般公開となり誰でもアクセスすることができます。

Microsoft は Q#言語とその SDK を開発、提供しています。SDK にはシミュレータも含まれているためローカルマシンでプログラムを実行することも可能です。

料金は IonQ のシミュレータで 1 時間あたり\$3,000 となっています。

また、Microsoft は独自の量子コンピュータを製造する研究開発も進めていますが、こちらは少し難航している模様です。2018 年の 3 月にデルフト大学のカウウェンホーフェンがマヨラナ粒子—その存在を予想したイタリアの物理学者にちなんで名付けられた—の存在を確認したという論文を発表します。Microsoft はこのマヨラナ粒子を利用した量子コンピュータの開発を進めてきましたが、現在この 2018 年の論文に疑問が投げかけられており、もしマヨラナ粒子が存在しないということになれば量子コンピュータ開発の計画が大きく後退することは避けられないでしょう。

## おわりに

昨年末には中国が独自に開発する光子を利用した量子コンピュータで量子超越性を達成したことを発表[10]しました。量子超越性とその先にある万能量子コンピュータの実現に向け、各社・各国開発競争を進める中でどうエラー耐性を獲得していくのか。今後も量子コンピュータ関連の話題から目が離せません。

[10]: https://science.sciencemag.org/content/370/6523/1460
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CPU Experiment at the University of Tokyo]]></title>
            <link>http://localhost:3000/blog/cpu-experiment</link>
            <guid>http://localhost:3000/blog/cpu-experiment</guid>
            <description><![CDATA["CPU Experiment" at the University of Tokyo (UT) CS undergrad course is one of the most famous (either in a good way or bad way) programmes among those who are concerned.

Here, I want to introduce what the programme is, and what kind of exciting stuff are going on there.]]></description>
            <content:encoded><![CDATA[
I note in passing that this is NOT based on my personal experience because I was not a CS student at UT. So apologies if I'm not very accurate in some details but I hope I don't miss the big picture.

## What's CPU Experiment

The programme is held in the autumn-winter term in the third year. CS undergrads are divided into a group of 3–5 people, and they'll be given a credit if they can successfully run the ray-tracing app written in MinCaml (a subset of OCaml), which is handed out at the introduction, at the end term presentation.

The rules and conditions have changed over time but basically what they need to do are:

- define ISA (original or based on PowerPC, MIPS, RISC-V, etc.)
- build a CPU core on a provided FPGA ([KCU105](https://www.xilinx.com/products/boards-and-kits/kcu105.html))
- write an assembler
- extend a [MinCaml compiler](https://github.com/esumii/min-caml) so it can compile the given ray-tracing app
- write a simulator to debug the compiler/assembler
- compile & run the required ray-tracing app on the self-built CPU

Each team member is assigned to one of the four tasks: CPU (core), FPU, simulator, and compiler. Conventionally the assembler seems to be done by a member who is responsible for the simulator.

What's cool about this on-the-job style programme I think is not only you can learn how to integrate the computer science knowledge you learned through the course, but also you need to finish the project in time collaborating with other team members.

## That's not the end of the story

This is where things get even more exciting.

For most of them it's not very difficult to run the required ray-tracing app, and actually they succeed in running the app in less than 3months.

How do they spend the rest of the term?

It's up to them how to use the time: Most of the teams try to optimise the system to amaze people by how fast they can run the app at the end term presentation.

The optimisation ideas include:

- design richer ISA
- introduce pipelining
- optimise compiler to reduce the number of instructions
- and so on

The current record seems to be 5s, which was made in 2017(\*), and generally speaking under 20s is considered to be brazing fast.

(\* The team realised multi-core CPU & the compiler that utilises it.)

However, some teams decide to venture into the wild adventure…

## Creative (crazy) ideas they try

As long as the ray-tracing app works, you can be credited for the course. There's nothing to prevent them from tackling their creative ideas such as:

- run an OS such as xv6, Linux, or even write an original OS
- run the ray-tracing app on the OS
- implement a single instruction that runs the ray-tracing
- etc.

There are lots of things involved to achieve these: To run an OS you'll need much richer ISA than just running the ray-tracing app. If you want to run the existing OS written in C on your original ISA, you'll need a C compiler that targets the ISA …and so on.

Some of them were realised by their exhaustive effort, while the others are yet to come true. So exciting, aren't they?

## Why am I writing this?

Some may wonder why I'm writing a post to introduce something I didn't do. Well, there are two reasons:

First of all, I think the programme is very cool and worth sharing. There are lots of blog posts written by students but most of them are in Japanese (see the next section), and only few resources are available in English for their cool stuff.

And the second one, rather personal one, is that I'm interested in tracing this "CPU Experiment" as my personal project. I've been working as a software engineer for almost 7 years. Thankfully I could grow a lot in this space, yet I've been feeling I missed chances to learn computer science basics. I thought it's a great idea to follow what they do to fill this gap.

I hope I can give updates on my project soon:)

## References

Even though most of them are written in Japanese, I'll leave some links for those who are interested so you can at least have a look at their code:)

2010

- [CPU 実験](https://wata-orz.hatenadiary.org/entry/20100319/1268998396)

2015

- [CPU 実験で自作 CPU に UNIX ライク OS (xv6) を移植した話](https://nullpo-head.hateblo.jp/entry/2015/03/24/205419)
- [EN][how we ran a unix-like os (xv6) on our home-built cpu with our home-built c compiler](https://fuel.edby.coffee/posts/how-we-ported-xv6-os-to-a-home-built-cpu-with-a-home-built-c-compiler/)
  - This is basically the same article as the one above but in English.
- [自作 CPU 向け C コンパイラをつくって OS 動かした話 (CPU 実験まとめ)](https://kw-udon.hatenablog.com/entry/2015/03/19/171921)

2016

- [東大理情名物の CPU 実験で毎週徹夜したお話（概要編）](https://medium.com/eureka-engineering/%E6%9D%B1%E5%A4%A7%E7%90%86%E6%83%85%E5%90%8D%E7%89%A9%E3%81%AEcpu%E5%AE%9F%E9%A8%93%E3%81%A7%E6%AF%8E%E9%80%B1%E5%BE%B9%E5%A4%9C%E3%81%97%E3%81%9F%E3%81%8A%E8%A9%B1-%E6%A6%82%E8%A6%81%E7%B7%A8-6cb8c155cb64)

2017

- [CPU 実験：マルチコアで並列実行するまで（コンパイラ係目線）](https://eguchishi.hatenablog.com/entry/2017/09/09/150229)
- [CPU 実験 2016 年度 D 班コア係（CPU 実験でマルチコア）](https://sueki743.hatenablog.jp/entry/2017/03/30/151822)

2018

- [東大の CPU 実験で自作コア上の自作 OS 上で自作シェルを動かした話](https://travelingresearcher.com/entry/2018/02/27/172417)

2019

- [RISC V に従う CPU の上で動作する OS を Rust で書く（CPU 実験余興）](https://moraprogramming.hateblo.jp/entry/2019/03/17/165802)

2020

- [Linux が動作する RISC-V CPU を自作した (2019 年度 CPU 実験 余興)](https://diary.shift-js.info/building-a-riscv-cpu-for-linux/)
- [CPU 実験が終わりました 〜コア係目線〜](https://misteer.hatenablog.com/entry/cpuex)
- [CPU 実験の振り返り (コンパイラ係目線)](https://okuraofvegetable.hatenablog.com/entry/2020/02/29/230201)

2021

- [CPU 実験振り返り(シミュレータ・FPU 係目線)](https://note.com/tomo_stleq/n/ncc8a1ff8ba20)
- [ああ CPU 実験](https://cfkazu.hatenablog.com/entry/2020/12/05/000416)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DDIA 読書ノート 【第10章】]]></title>
            <link>http://localhost:3000/blog/ddia-chapter-10</link>
            <guid>http://localhost:3000/blog/ddia-chapter-10</guid>
            <description><![CDATA[第10章はバッチ処理について、MapReduceを中心に展開していく。]]></description>
            <content:encoded><![CDATA[
第 9 章が難しかったこともあり、読書ノートがまだ書ききれてないんですが、なんとか完成させようとは思っているのでとりあえず 10 章の話をどうぞ。。

<aside>
<p><a href="https://open.spotify.com/show/0J8LZwfrGB9BJLihy4Ldb1">London Tech Talk</a> というポッドキャストを運営している <a href="https://twitter.com/tommyasai">@tommyasai</a> さん・<a href="https://twitter.com/kenwagatsuma">@kenwagatsuma</a> さん主催で DDIA の輪読会が行われており、それに向けて DDIA を読んでいるのでその際のまとめです。今回は第 10 章。</p>
<p>理解したこと、考えたことをそのまま書き落としていくので読み物としては読みづらいかと思いますがその点は悪しからず。<p>
</aside>

第 10 章からはチャプターが変わり、チャプター 3。このチャプターではそれぞれに特性の異なるシステムを組み合わせてより大きなシステムを構築する方法が紹介されている。

システムは単体で全てのニーズに応えることが難しい and/or 非効率なため、どう組み合わせるか、といういうのは見落とされがちだが重要なトピックになってくる。

その中で第 10 章はバッチ処理について。MapReduce に関する話題を中心に展開されている。

## Unix 再訪

唐突に感じるところもあるがまず取り上げているのが Unix の哲学とそれに基づいた Unix ツール群、というのも MapReduce はこの Unix の概念や教訓と共通する部分が多いため、馴染みのある Unix がまず最初に取り上げられている。

特に重要なのが Unix がファイルを介した、**一様なインターフェイスを持つ**という点で、MapReduce も同様に分散ファイルシステムをインターフェイスとして、それぞれの Job がお互いを意識することなく入出力をやりとりできるようにデザインされている。

Unix の考え方は様々なところで引用されており、馴染みが深いが、ここにも顔を出すその影響の大きさに改めて驚いた。

この Unix の特徴を踏まえた上で MapReduce の詳細説明に入っていく。

## MapReduce について

MapReduce にはその名にあるように、mapper と呼ばれるステップと reducer と呼ばれるステップがある。

mapper では先に触れたように分散ファイルシステム上のファイルを通して受け取る入力のデータを key-value のペアにマッピングする。この key-value のペアはソートされたのち key ごとに同一の reducer に送られる。

reducer では mapper から送られてくる key-value を元に好きなことをすればよく[^1]、例えば検索エンジンのインデックス構築であったり、出力をそのまま key-value ストアのデータとして利用するといった用法があるが、最もよく使われる使い道はなんといっても別の Job のインプットにする、ということだ。

先にも触れたように MapReduce は分散ファイルシステムという一様なインターフェイスを持つ。reducer の出力は入力として受け取ったのと同様に分散ファイルシステムに書き出されるため、次の Job はその入力が前の Job の出力である、ということを知ることなく実行できる。これはまさに Unix ツールがファイルを介して連携している点と対応している。

[^1]: といってもそれは冪等な操作に限る。Job が失敗した際、失敗した計算のみを自動で再実行することで耐障害性を高めているため、実行する度に結果が変わる、というのでは困ってしまう。

このように、ユーザーは mapper と reducer で何をするかだけを考えればよく、I/O や分散ネットワークについて頭を悩ませることなく Job と Job を繋ぎ合わせて大量のデータを効率よく処理することができるのが MapReduce の特徴である。

MapReduce は大量のデータに対するバッチ計算処理のフレームワークを提供したわけだが、そのことにより、収集したデータはひとまずどこかに保存してしまい、それを処理するのは別のタスクとして切り出す、というアプローチが広まった。これによりデータの収集よりもデータの解釈に重きが重要視されるきっかけになったり、生のデータの方が加工されたデータよりも有用だ、という考え方(sushi principle、寿司原則と呼ばれる)に繋がった。

## MapReduce から時を経て

と、このように MapReduce はデータ処理に対するアプローチを変える画期的なアイデアだったわけだが、論文が発表されたのは 2004 年。それから月日が経ち、MapReduce のある意味シンプルすぎる考え方から発展させて、より最適化が図られた手法が次々と開発されており、今日では MapReduce をそのまま利用する、ということは少なくなっている。

具体的には、MapReduce は Job の都度ファイルへの書き出しが行われるため、読み書きのオーバーヘッドが発生する。Apache Spark や Apache Tez では mapper の出力を別の mapper に繋げたり、reducer の出力を直接次の mapper に繋げたりすることで、DAG(Direct Acyclic Graph: 有向非巡回グラフ)を構築、全体を一つの Job とすることでファイル書き出しのオーバーヘッドを減らす工夫がされている。

この他にも中間層の不要なソーティングを無くしたり、耐障害性を高めるなど MapReduce を元により最適なバッチ処理のプラットフォームが実現されている。

## 次章、ストリーム処理

このように MapReduce を通して分散バッチ処理について紹介されてきたが、次章のトピックはストリーム処理となる。両者の１番の違いは処理するデータのサイズが事前にわかるかどうか、という点だ。

今回も学びが多かったので、次章もとても楽しみ。

この章に限らず DDIA の本それ自体に対してもそう言えるのだけど、これまでキャリアがかなり計算・ロジック(あるいはオンラインシステムとも言えるかも知れない)の方に偏っているので、データの方の知識・経験も増やしていきたいと思っている。

もしこのあたりのトピックに関しておすすめの本やリソースなどがあれば(Hadoop などの個別トピックでも全然いいので)紹介してほしいです！
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Does integer division overflow?]]></title>
            <link>http://localhost:3000/blog/division-overflow</link>
            <guid>http://localhost:3000/blog/division-overflow</guid>
            <description><![CDATA[In this short post, I'm going to introduce integer overflow and small fun fact that causes overflow in seemingly impossible situation.]]></description>
            <content:encoded><![CDATA[
## What's integer overflow?

> In computer programming, an integer overflow occurs when an arithmetic operation attempts to create a numeric value that is outside of the range that can be represented with a given number of digits – either higher than the maximum or lower than the minimum representable value. — [Wikipedia — Integer overflow](https://en.wikipedia.org/wiki/Integer_overflow)

As you may know, addition of two integer numbers can overflow. For example addition of two `int8` number 127 and 1 overflows, and doesn't return mathematically correct answer.

```go
package main

import "fmt"

func main() {
        var i, j int8 = 127, 1
        fmt.Printf("127 + 1 = %d", i+j)
}
```

This Go programme outputs `127 + 1 = -128`. For subtraction, if you think about `127 - (-1)`, this is going to overflow as this is the equivalent expression to the addition we just saw.

Likewise, multiplication of two integer numbers can overflow.

```go
package main

import "fmt"

func main() {
        var i, j int8 = 64, 2
        fmt.Printf("64 * 2 = %d", i*j)
}
```

This outputs `64 * 2 = -128`.

The reason is because 127 is the biggest number `int8` can represent and any calculation resulting in bigger number than this overflows returning unexpected result.

Here is a question: Can division of two integer numbers ever overflow? If yes, which combination of two integers make it happen?

## Integer division overflow

From mathematical point of view, if the divisor is either bigger than or equal to 1, or smaller than or equal to -1, the absolute value of the quotient is never going to be bigger than that of the dividend, and for any `int8` number this condition is satisfied. So it doesn't seem possible to make overflow happen with two `int8` numbers.

…However, yes it is possible, as you may have guessed.

What do you think is the output of this Go programme?

```go
package main

import "fmt"

func main() {
        var i, j int8 = -128, -1
        fmt.Printf("-128 / -1 = %d", i/j)
}
```

Mathematically correct answer is `128`…but wait, **wasn't the biggest number `int8` can represent 127??**

That means this calculation overflows and the output is `-128 / -1 = -128`.

The reason why this happens is while `int8` can represent `2^8 = 256` different numbers, 0 has to be included, which means odd number `255` is left to represent both positive and negative numbers. Most of the computing systems adopt a technique called _two's complement_ to represent signed numbers[^1] and with that technique `int8` is going to have **127 positive numbers and 128 negative numbers**.

As a result by dividing the biggest negative number by -1, we can cause overflow in integer division.

I'd like to know more of this kind of "fun facts", so please share if you know. In the next post I'd like to talk about what two's complement is, and why it is so widely used in the computing systems.

[^1]: Signed numbers: Numbers that consist of both positive and negative numbers
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[シェーダーについて (1)]]></title>
            <link>http://localhost:3000/blog/about-shader-1</link>
            <guid>http://localhost:3000/blog/about-shader-1</guid>
            <content:encoded><![CDATA[
WebGL について調べていく中で、シェーダーというのが出てくる。

チュートリアルなどでは「シェーダーとは陰影処理をするプログラムである」などと説明はそこそこに、「ではシェーダーを書いてみましょう」となるのだが、シェーダーの理解が覚束ないためサンプルのプログラムが何をしているのかいまいちよく掴めない、ということがよくあった。

シェーダーを理解する上で非常に役に立ったのが、その全体像を把握することだった。

そもそもやりたいことというのは、手元に 3D のデータがあり、これを 2D である画面への描画、つまりピクセルごとの色情報の列に変換することだ。ちなみに入力である 3D データがどこからくるかというと、Blender などのソフトを使ってモデリングという作業を行うわけだが、そういったことを含めて 3DCG という技術全体が成り立っている。

話を戻して描画に関して、この 3D から 2D への変換を一般的にレンダリングと呼ぶ。さらにこのレンダリングはパイプライン処理、つまりタスク全体をより小さいサブタスクに分けた上でバケツリレー形式で処理されるため、レンダリングパイプラインまたはグラフィックスパイプラインと呼ばれる。

シェーダーはこのレンダリングパイプラインの中で陰影をつける役割 (に今は限られず、様々な処理を行う) を担うのだが、ポイントは**プログラム可能である**というところだ。

元々レンダリング処理は全て CPU 上で行なわれていたが、専用ハードウェアを用意することで高速化が図られた、というのが GPU の始まりである。

<aside>
時とともにGPUが担当する処理が増え、処理内容も複雑化、さらには機械学習を中心にレンダリング以外の用途への転用と発展し、年を増すごとにGPUの重要度が大きくなっている。
今年 5 月には GPU の老舗、NVIDIA の時価総額が 1 兆円を突破するなど、非常にホットな領域である。
</aside>

こうして登場した初期の GPU は機能が全て固定されて提供されていた。そのため新しい処理を実装したい場合にはハードウェアのリリースを待たなければいけなかった。

3D 技術が急速に発展し、要求も多様化するのに伴い、ハードウェアアップデートを待たずに機能を実装したいというニーズが生まれた。GPU ベンダーはハードウェアの一部をプログラム可能にすることでこの要求に応えた。

このプログラムこそが、「陰影処理をするプログラム」であるシェーダーである。

---

先にも書いたが、近年シェーダーの守備範囲は陰影処理に限られなくなっている。

シェーダーが実際にレンダリングパイプラインの中でどういった処理を行なっているのかについて、次回見ていきたい。

では、また。
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NFT — 可能性と問題点]]></title>
            <link>http://localhost:3000/blog/nft-potential-and-problems</link>
            <guid>http://localhost:3000/blog/nft-potential-and-problems</guid>
            <description><![CDATA[NFTの話題が連日テックニュース界隈を賑わせています。きっかけはデジタルアート作品のNFTに$69mの値段がついたことです。作品は Beepleというアーティストによるもので、取引はNFTを扱うオークションサイトのChristie'sで行われました。その後もTwitterの創業者であるJack Dorseyによる、Twitter初のツイートに$3mの値段がつくなど、話題がつきません。
NFTはデジタル作品を手がけるアーティストを支援する強力なツールとなる可能性がある一方でいくつかの問題も抱えています。ここではNFTのもつ可能性と問題点についてまとめます。]]></description>
            <content:encoded><![CDATA[
まずはじめに NFT とはなにか簡単に説明します。

## NFT とは

NFT とは Non-Fungible Token の略で、ブロックチェーンを応用した技術のひとつです。日本語では非代替性トークンと訳されますが、従来のブロックチェーン技術との違いはまさにその非代替性にあります。

ブロックチェーンは基本的にはデータベースです。ビットコインのような仮想通貨を例にとると、そのブロックチェーン上には誰と誰との間でいくらの移動があったかという取引情報が保存されています。現実世界の通貨においてあなたが持つ 100 円玉と僕が持つ 100 円玉の区別をつける必要がないのと同様に、ビットコインであってもそれぞれのアセットの区別をつける必要はありません。これを代替可能(=fungible)と呼びます。

その一方で世の中には代替不可能なものも存在します。一例としてアート作品を想像してみてください。モナリザのコピーは世の中に多数存在しますが、オリジナルはルーブル美術館にある一つだけです。あなたの持つモナリザのコピーをルーブル美術館のオリジナルと交換することはできません。これを代替不可能(=non-fungible)であると言います。NFT を使ったブロックチェーン上にはそれぞれのアセットを表すユニークな ID とその所有者のマッピングが保存されており、そのブロックチェーンを見ればある作品を所有しているのが誰なのか知ることができます。また、仮想通貨と同様にその所有権を移動させることも可能です。

アート作品がしばしばオークションを通して売りに出されるのと同様に、NFT でもオークションを通して所有権が売買されます。それこそが今 NFT 界隈で起こっていることです。ただし違いはそこで売買されているのがモナリザのような絵画ではなく、画像や映像といったデジタルデータだということです。

## なぜ NFT が盛り上がっているか

デジタルなデータが物理的に存在するモノと圧倒的に異なる点は、オリジナルをコストゼロで簡単にコピーできることです。cmd+c を押すだけで本物と全く同じものを無数に作り出すことができます。それゆえ、これまでデジタルデータは実物のように「所有する」ということができませんでした。

そこに出現したのが NFT です。

さきほど説明したように、NFT を使うことによりブロックチェーン上に"誰が何を所有するか"の情報を書き込むことができ、一度書き込まれた内容を勝手に書き換えることはほぼ不可能です。NFT によりデジタルデータを実物のアート作品と同様に、コレクションとしてあるいは投資対象として「所有する」ということが可能になった、というのが NFT がこれだけ盛り上がっている大きな理由です。

このことは NFT がデジタル作品を手がけるアーティストを支援する強力なツールとなる可能性を示しています。これまで著作権という保護はありながらも、無断でコピーされたものから生み出された利益がその作成者に還元されることはありませんでした。残念ながら NFT はその問題自体に解決策を与えてくれるものではありません。しかし「NFT を通して所有権を売る」という、アーティストが収入を得る新たな手段を生み出しました。加えて多くの NFT プラットフォームでは売買のたびに出品者に手数料が入る仕組みを導入しており、売買が行われる限り継続的に収入を得ることができるという実物の取引にはないメリットもあります。

ここまでをみるとデジタルな創作物に正しい評価が与えられるすばらしい発明、と考えられます。しかし実態は少し異なっているようです。

## 「アーティストがお金を稼ぐ手段」が増えてハッピー？

一見すばらしい仕組みにみえる NFT ですが、問題もいくつか抱えています。

### 1. 誰でも出品できるという欠陥

先述のとおり一度ブロックチェーンに書き込まれた内容は変更することができません。そのため正しい情報を書き込むことが大切です。デジタルデータを扱う NFT の場合は「クリエイター＝出品者」であることが求められますが、これを保証してくれるはずの認証システムの信頼性にはプラットフォームによってばらつきがあります。Rarible という NFT プラットフォーム上では、アーティストがある日起きたら自分の作品が勝手に出品され、売買さえも完了していたという事例[3]が実際に発生しています。また別の NFT プラットフォームである OpenSea は「爆発的に増加した出品リクエストに対応する」と言う名目で認証システムを一切無くしました[3]。あるいは盗作や二次創作物などの出品を防ぐ有効な手段もありません。

クリエイターに新たな収入源を与える一方で、違法者やモラルを無視した人にも等しくその利便性が与えられては仕方がありません。

[3]: https://www.theverge.com/2021/3/20/22334527/nft-scams-artists-opensea-rarible-marble-cards-fraud-art

### 2. 「所有権」というものの曖昧さ

NFT における「所有権」とは実際に何を指すのでしょうか。ブロックチェーン上の各ブロックにはそれがどういった作品なのかという情報が書き込まれなければいけません。しかしブロックチェーンに書き込むことができるデータのサイズには限りがあり、作品をそのままデータとして記録することができません。そこで多くのプラットフォームでは作品への URL(正確には作品の URL を含むメタデータへのリンク)だけをブロック上に記録します。この作品へのリンクをもって所有権というものが成立しています。

しかし URL を使うことには大きな問題があります。もしそのリンクをもつサービスが停止してしまったらどうなるでしょうか。ブロックチェーンは残りますがそこに書き込まれたリンクは無効となり、もうアクセスすることはできません。何を所有しているかわからない「所有権」だけが残ることになります。所有権を URL を使って表現することには信頼性という点で疑問が残ります。

いくつかのプラットフォームではこの問題に対応するため、ブロックに IPFS のトークンを書き込む方式を採用しています。IPFS とは Inter-Planetary File System の略で、ファイルを分散ホスト上に展開し、peer-to-peer システムによりアクセス可能にする技術です。IPFS 上のファイルは URL のような場所を特定する情報は持たず、唯一性をもつトークンのみが与えられます。そしてそのファイルをホストするサーバーが一つでも存在する限りファイルが失われることはありません。

しかしこの IPFS にも問題があります。作品のメタデータは分散して保存されており失われることがありませんが、肝心の作品データが特定のサーバー上にしか存在しないということがあります。URL を使ったケースと同様に、そのサーバーを運用するサービスが停止した時点でそのデータにアクセスすることはできなくなります。

NFT のコンセプトの発案者 Anil Dash はこのブロックチェーンにリンクだけを書き込む方式を"shortcut"としています[4]。その"shortcut"はコンセプトの発案から 7 年たった今も使われ続けています。 NFT における「所有権」とは現状、いつなくなるかわからないとても脆弱なものです。

[4]: https://www.theatlantic.com/ideas/archive/2021/04/nfts-werent-supposed-end-like/618488/

### 3. 環境負荷という問題

ブロックチェーンに新たなブロックを書き込む際の環境負荷という点も見逃せません。多くの NFT プラットフォームではデジタルデータを Ethereum という、すでにあるブロックチェーン上に書き込んでいます。この Ethereum というブロックチェーンは、データの改ざんを防ぐためにプルーフオブワークという仕組みを採用しています。このプルーフオブワークにはマイニングという作業が必要になるのですが、このマイニングにはとてつもない量の計算が必要となります。大量の計算にはそれに見合うだけの電力を必要とします。Ethereum が年間に消費する電力量はベラルーシが年間に消費する電力量に匹敵するといわれています[5]。

これは Ethereum というブロックチェーンが抱える問題であるため、NFT 自身の問題とはいえません。しかし NFT が注目を浴びることで利用者が増え、Ethereum でのマイニング作業がより必要となることで結果的に環境に与える負荷が高まるのは間違いありません。事実今年に入ってから Ethereum の電力消費量は激増しています[5]。

[5]: https://digiconomist.net/ethereum-energy-consumption

## NFT が向かう先

「NFT を通したアート作品が\$69m で落札！」 といったセンセーショナルな話題に振り回されていますが、NFT は本来アーティストを保護し、支援するための仕組みだと Anil Dash は述べています。NFT という技術が大きな可能性を秘めているのは間違いありませんが、出現してから日が浅く、未だ多くの課題を抱えているのも事実です。一過性の話題として消費されるにとどまらず、技術が円熟し有益なものとして利用可能になるのを期待しています。

---

## 参照

- [Beeple sold an NFT for \$69 million](https://www.theverge.com/2021/3/11/22325054/beeple-christies-nft-sale-cost-everydays-69-million)
- [Jack Dorsey’s first tweet sold as an NFT for an oddly specific \$2,915,835.47](https://www.theverge.com/2021/3/22/22344937/jack-dorsey-nft-sold-first-tweet-ethereum-cryptocurrency-twitter)
- [THE CLIMATE CONTROVERSY SWIRLING AROUND NFTS](https://www.theverge.com/2021/3/15/22328203/nft-cryptoart-ethereum-blockchain-climate-change)
- [NFT MANIA IS HERE, AND SO ARE THE SCAMMERS](https://www.theverge.com/2021/3/20/22334527/nft-scams-artists-opensea-rarible-marble-cards-fraud-art)
- [YOUR MILLION-DOLLAR NFT CAN BREAK TOMORROW IF YOU’RE NOT CAREFUL](https://www.theverge.com/2021/3/25/22349242/nft-metadata-explained-art-crypto-urls-links-ipfs)
- [Do You Really\* Own Your NFT? Chances Are, You Don’t](https://thedefiant.io/do-you-really-own-your-nft-chances-are-you-dont/)
- [NFTs Weren’t Supposed to End Like This](https://www.theatlantic.com/ideas/archive/2021/04/nfts-werent-supposed-end-like/618488/)
- [The Non-Fungible Token Bible: Everything you need to know about NFTs](https://opensea.io/blog/guides/non-fungible-tokens)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[WebGLについて]]></title>
            <link>http://localhost:3000/blog/about-webgl</link>
            <guid>http://localhost:3000/blog/about-webgl</guid>
            <description><![CDATA[アニメーションがかっこいいサイトに WebGL という技術が使われているらしいということは小耳に挟んでおり、WebGL について気にはなってはいたものの、ちゃんと調べたことはまだなかった。
この度仕事で使う可能性が出てきたということで、少し調べてみたのでまとめてみた。]]></description>
            <content:encoded><![CDATA[
## WebGL とは

WebGL は Web Graphics Library の略であり、その名の通りウェブアプリ、つまりは **JavaScript でグラフィック操作を行うためのライブラリ**である。

そもそもアプリから 2D/3D の描画をする際には OpenGL という C 言語のライブラリが存在し、広く使われている。
ネイティブなアプリであれば OpenGL を使えば良いのだが、当然ながらウェブ上のアプリからも綺麗なアニメーションを表示したいという要求が出てくる。

そこで 2006 年に Firefox ブラウザを開発する Mozilla がウェブ上で 3D 描画をするプロトタイプを発表。2009 年、そこに OpenGL の仕様を策定しているクロノスグループが参加しワーキンググループが発足、主要なブラウザベンダーである Apple や Google も参加したことにより標準化の動きが生まれた。

WebGL は OpenGL(より正確には組込みシステム用に定義されたサブセットである OpenGL ES)をベースに定義されており、この仕様をもとに各ブラウザが機能を
提供している。

2017 年にリリースされた WenGL 2.0 が最新の API となっており、2023 年 6 月現在モバイルを含む主要な各ブラウザでサポートされている。(https://caniuse.com/webgl2)

## WebGL のメリット

ウェブ上で 2D/3D を描画する方法にはそれまで Flush Player や Unity Web Player といった技術が存在した。

これらの既存技術はブラウザへのプラグインという形で提供されるため、ユーザーのブラウザにプラグインがインストールされていないと動作しないという欠点がある。
セキュリティの観点からプラグインの導入が敬遠されていることもあり、せっかくかっこいいウェブサイトを作っても動作環境が整わない可能性があった。

これに対し、WebGL は JavaScript の API であり、仕様策定のワーキンググループに主要なブラウザベンダーが参加していることもあり、**多くのブラウザに標準でサポートされている**。

「GPU が OpenGL をサポートしていないといけない」という制限はあるものの、WebGL の登場により JavaScript さえ動けばほとんどのブラウザ環境で 2D/3D が好きなように描画できる状況が生まれた。

## WebGL のその先

現在 WebGL を置き換える次世代のインターフェイスとして WebGPU が提案されている。
(2023 年 6 月現在デスクトップ版の Chrome のみが正式にサポートしている。)

そもそも OpenGL は GPU を利用することで描画演算を高速に処理することを実現しているが、近年のハードウェア性能の向上や機械学習の盛り上がりを背景に、この GPU を取り巻く環境が大きく変化し、1990 年代前半に登場した**OpenGL では満足いかないケースが増えてきた**。

より具体的には、OpenGL は抽象度が高い API のため、GPU 利用を細かくチューニングすることが難しい。加えて、機械学習を中心に、GPU をグラフィックスだけでなく、一般的な計算リソースとして利用したいというニーズが高まりつつある(GPGPU: General Purpose GPU と呼ばれる)。

そこで GPU をより効率的に利用したい各社がより低レベルの API を発表する流れが生まれた。GPU チップメーカーの NVIDIA から CUDA(CUDA はコンピューティングプラットフォーム全体の名称であり、API はその一部)が、Apple からは Metal、そしてクロノスグループからも Vulcan という仕様がそれぞれ発表されている。

これらの動きを背景に、WebGL も次世代 API へと置き換える流れが生まれ、WebGPU という仕様が登場。標準化の動きが進んでいる。

---

WebGL とは何なのか、WebGL が生まれた背景、そしてこれからの動きについて追ってきた。

このほかのトピックに Three.js といった WebGL をよりラクに利用するためのライブラリや、シェーダーを記述するための言語である GLSL などがあり、次回以降この辺りにも踏み込んでいけたらと思っている。

それではまた。

## References

- [Wikipedia — WebGL](https://ja.wikipedia.org/wiki/WebGL)
- [WebGPU がついに利用可能に WebGL 以上の高速な描画と、計算処理への可能性](https://ics.media/entry/230426/)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ray Tracer on Web]]></title>
            <link>http://localhost:3000/blog/ray-tracer-on-web</link>
            <guid>http://localhost:3000/blog/ray-tracer-on-web</guid>
            <description><![CDATA[I have been working on writing a ray tracer in Rust in the last few weeks. This post is to show off what I could achieve:)]]></description>
            <content:encoded><![CDATA[
First things first: demonstration. Try clicking the Run button below. It's going to take a bit while until the result pops up, so please be patient:)

(NOTE: Web Worker must be [supported](https://caniuse.com/webworkers) on your browser.)

<ray-tracer-canvas></ray-tracer-canvas>

It didn't load the image but **generated it on your device** using the power of Web Assembly. Because it has some random factor, it produces slightly different images every time you run it.

It's cool, isn't it?

I'm first going to touch on what ray tracer / ray tracing is for those who are not familiar with it. Then look at details of how I ran it in the browser.

<aside>
Many thanks to <a href="https://twitter.com/Peter_shirley">@Peter_shirley</a> who wrote a phenomenal tutorial <a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">Ray Tracing In One Weekend</a>. I just followed his tutorial using Rust instead of C++.
</aside>

The source code is available on [GitHub](https://github.com/KentaKudo/wasm-raytracer), and also published as a [NPM package](https://www.npmjs.com/package/wasm-raytracer).

## What's ray tracing?

Ray tracing is a technique used to render a scene. Pay close attention to the shadows and the reflection on water, walls, floors, and glasses in this video.

<div class="embed mb">
  <iframe
    width="560"
    height="315"
    src="https://www.youtube.com/embed/Xf2QCdScU6o"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen
  ></iframe>
</div>

You may find the scene rendered by ray tracing more natural.

While ray tracing is computationaly more expensive than the widely used method _rasterisation_, it's been increasing its presence in movie and game rendering with the help of the latest advance in GPU technology.

Programatically, it calculates the colour of each pixel in the image from top left to right bottom tracing "rays" from a "camera" to the "scene".

### Scene

A scene is the world where all the objects, for example cars, lights, buildings, and so on, are placed. My ray tracer only has simple spheres as they're easy to model, but you can put literally any objects in any positions if you manage to model it.

You can add other factors to objects to make it look more real. One such factor is a **material** the object is made of. Depending on the material, the object behaves differently when a ray hits. Materials supported by my version of ray tracer are:

- Lambertian — a material that reflects the ray to a random direction,
- Metal — a material that reflects the ray to the direction calculated from the incoming ray's angle, and
- Dielectric (glasses) — a material that refracts the ray to the inside of the object.

<nuxt-image src="blog/ray-tracer-on-web/materials.png" alt="Lambertian casts a ray to a random direction, Metal reflects, Dielectric refracts" caption="Each material baheves differently when a ray hits it"></nuxt-image>

Once you define a scene, the next thing you need to do is to specify where you look at the scene from.

### Camera

A camera is your eyes. All the rays start their journey from the camera. It's defined with two main parameters:

- position — the place where you look at the scene from, and
- atitude — the direction of its gaze

In addition to them, you also need to configure
(1) a viewport which has the same aspect ratio as the final image, think it as a window which you look at the scene through, and
(2) the distance between the camera and the viewport, which is called "focal length".

Once the camera is positioned, it's ready to start casting rays.

### Ray

A ray is casted from the camera to the direction of the pixel it's interested in at the time. The casted ray may or may not hit objects placed in the scene. If it hits something, it first records the color of the object surface. Then, it produces another ray, the direction of which depends on the material of the object as explained in the scene section, from the hit point.

The tracing of the ray stops either
(1) when the ray goes deep into the shadow or
(2) when it doesn't hit anything anymore, which means it reaches the ambient light (or sky so to speak).
The final colour of the pixel is calculated at that point.

<nuxt-image src="blog/ray-tracer-on-web/ray.png" alt="A ray is cast from a camera to the scene through a frame in between them calculating the final colour of the pixel" caption="Tracing the journey of the ray"></nuxt-image>

This is a quick explanation of what the ray tracer is. See the [_References_](#references) section to continue reading more about ray tracing if you're interested.

Now, let's talk about what I did to run it on web browser.

## Ray tracer + Web Assembly

The first thing I had to do was make a decision on how JavaScript and Rust should communicate each other; more precisely, how to pass images from Rust to JavaScript.

### Canvas API

To draw an image on the screen, I used the \<canvas\> HTML element. Its `putImageData()` API receives input image as an `ImageData` object, and the image data is given as `Uint8ClampedArray`, which is expected to be an array containing the pixel data in the RGBA order.

<nuxt-image src="blog/ray-tracer-on-web/canvas-api.png" alt="Uint8ClampedArray is used for ImageData then passed to canvas HTML element."></nuxt-image>

Long story short, the goal is to return a properly serialised `Uint8ClampedArray` from the Rust programme.

### wasm-bindgen

[wasm-bindgen](https://github.com/rustwasm/wasm-bindgen) is a library that bridges the gap between JavaScript and the Web Assembly module written in Rust.

What I needed to do so the `render` function can be called from JavaScript and return `Uint8ClampedArray` was to add a `#[wasm_bindgen]` annotation and modify the return type:

```rust
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub fn render(width: u16, height: u16) -> Uint8ClampedArray {

  // ...setup a scene and a camera

  let mut img = vec![];

  // ...build image

  // convert from [u8] to Uint8ClampedArray using Into trait.
  img[..].into()
}
```

This is basically everything I needed to run the ray tracer in web browser, but there was one issue when I tried it: the browser got stuck while running the ray tracer. What I was missing was to follow this advice:

> The main thread in a browser cannot block. This means that if you run WebAssembly code on the main thread you can never block, meaning you can't do so much as acquire a mutex. This is an extremely difficult limitation to work with on the web, although one workaround is to run wasm exclusively in web workers and run JS on the main thread. It is possible to run the same wasm across all threads, but you need to be extremely vigilant about synchronization with the main thread. —— [Parallel Raytracing#caveats](https://rustwasm.github.io/wasm-bindgen/examples/raytrace.html#caveats)

Because ray tracing takes some time and blocks the thread where it runs, running it on the main thread is a no-no; I made a first encounter to Web Worker...!

### Web Worker

Regardless of its unfriendly interface ([Comlink](https://github.com/GoogleChromeLabs/comlink) can be a help for it), using Web Worker is not super complicated.

The main thread needs to (1) instantiate a worker specifying the path to the worker file, (2) invoke it passing arguments via `postMessage` API, and (3) listen to the result by registering an `onmessage` callback:

```js
const w = new Worker("../path/to/the/worker/file");
w.postMessage([WIDTH, HEIGHT]);
w.onmessage = ({ data: rendered }) => {
  const imageData = new ImageData(rendered, WIDTH);
  canvas.getContext("2d").putImageData(imageData, 0, 0);
};
```

The worker in return defines an `onmessage` method, calls the wasm function, and pass back the result using `postMessage` API:

```js
onmessage = async function (e) {
  const { render } = await import("wasm-raytracer");
  const rendered = render(e.data[0], e.data[1]);
  postMessage(rendered);
};
```

This was all I needed to run ray tracer on a dedicated thread.

Before closing the section, I address the performance issue I tried to solve.

### Multi-threading

Even though now it's separated from the main thread, running it on a single thread takes a bit of time. To improve performance, I looked at options to run it on multiple threads.

The calculation of each pixel can be done in isolatation from each other, and _fork-join parallelism_, which is to _fork_ to start a new thread and to join to _wait_ for it to finish, can be applied using libraries such as [Rayon](https://github.com/rayon-rs/rayon).

Even though it wasn't difficult to turn the programme into the multi-thread version, there were a couple of challenges to run it in the browser environment:

- WebAssembly threads are not supported by [some browsers](https://webassembly.org/roadmap/) yet.
- To enable it, [COOP and COEP headers](https://web.dev/coop-coep/) need to be configured properly, which is not possble for GitHub pages where this blog is hosted as of today.
- Because Rust's wasm build target doesn't assume it's going to be run in browsers, the standard library `std::thread` is not aware of Web Workers that enable WebAssembly threads on browser environments.
- The [`wasm-bindgen-rayon` library](https://github.com/GoogleChromeLabs/wasm-bindgen-rayon) enables compiling multi-thread code written with the Rayon library into threads aware wasm, but only with the nightly version of Rust.

For those reasons, I gave up multi-threading for now hoping I can introduce it in the near future.

## Why ray tracer?

This is kind of a detour from my journey of the [CPU experiment](/blog/cpu-experiment) aimed at understanding what ray tracer is and how to implement it (+ not to mention learning Rust). Next challenge is to migrate the ray tracer programme to the Hack/Jack system I built while I was reading the book I introduced in the last section of the [The Power of NAND](/blog/the-power-of-nand) post. Bye for now:)

## References

- [Ray Tracing In One Weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.html)
- [The joy of building a ray tracer, for fun, in Rust.](https://blog.singleton.io/posts/2022-01-02-raytracing-with-rust/)
- [Writing a Raytracer in Rust](https://bheisler.github.io/post/writing-raytracer-in-rust-part-1/)
- [Shaking Off the Rust 2: Ray Tracing in WebAssembly](https://clayto.com/2021/07/shaking-off-the-rust-2-ray-tracing-in-webassembly/)
- [Parallel Raytracing](https://rustwasm.github.io/wasm-bindgen/examples/raytrace.html)
- [Using WebAssembly threads from C, C++ and Rust](https://web.dev/webassembly-threads/)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Subtraction as Addition]]></title>
            <link>http://localhost:3000/blog/subtraction-as-addition</link>
            <guid>http://localhost:3000/blog/subtraction-as-addition</guid>
            <description><![CDATA[In modern computing system, a technique called two's complement is used to represent negative integers. The reason it's so commonly used is it makes computing system much simpler. In this post, I'd like to show what two's complement is and how it helps keeping it simple.]]></description>
            <content:encoded><![CDATA[
To make the discussion simple I'm going to use 4 bit integer, `int4` throughout the post.

NOTE: I expect you to have a basic understanding of binary number system. If you're not familiar with it, take a moment and have a look at [this article](https://www.mathsisfun.com/binary-number-system.html) first.

## What's two's complement?

It's one way of representing signed integers[^1]. `int4` can represent `2^4 = 16` different numbers, and two's complement assigns 16 integers ranging from -8 to 7 to each bit pattern. The mapping rules go like this:

- Map from 0 to 7 to binary numbers starting from `0000` to `0111`. This is same as how we represent positive integers in binary.
- For negative numbers, assign the smallest negative number -8 to `1000`, then assign -7 to the one bigger binary `1001`, -6 to `1010` …and so on up to `1111`. `1111` is going to be -1.

If they aren't very clear, have a look at the table showing full mapping below and examine the rules again.

| Decimal | Binary |
| :-----: | :----: |
|    7    |  0111  |
|    6    |  0110  |
|    5    |  0101  |
|    4    |  0100  |
|    3    |  0011  |
|    2    |  0010  |
|    1    |  0001  |
|    0    |  0000  |
|   -1    |  1111  |
|   -2    |  1110  |
|   -3    |  1101  |
|   -4    |  1100  |
|   -5    |  1011  |
|   -6    |  1010  |
|   -7    |  1001  |
|   -8    |  1000  |

Just by looking at it, it doesn't make much sense why it does such a tricky mapping for negative numbers. There's a reason powerful enough to justify this trickiness.

## Why is it so widely used?

The reason why two's complement is widely used is it makes hardware simple in a way that **it enables hardware to use the addition unit to also calculate subtraction**.

You may be surprised to know that computers don't do subtraction. What it actually does instead is addition of the negative number. For example, `5 - 3` is calculated as `5 + (-3)` inside a computer. By converting subtraction to addition, it's now free from minding how to go about subtraction, which is a big deal from hardware engineering point of view.

However, we need to carefully pick a way to represent negative numbers in order to get correct answers. Two's complement is one of such representations[^2] that makes it happen. Let's look at an example.

Let's do `5 + (-3)`. In two's complement 5 and -3 are represented as `0101` and `1101` respectively. The addition of these two is `0101 + 1101 = 0010`. If you look back at the table in the previous section, you can find it represents 2, which **corresponds to the answer of `5 + (-3)`**.

You may still wonder "so what? what's so special about this?" because my first impression was like that. Let's see what happens if it did NOT use two's complement.

### Another way of representing minus numbers

I'm going to use the first bit solely as a flag that indicates sign: 0 to be positive, and 1 to be negative. The rest of three bits are used to represent number part, for example, `0001` is 1 and `1001` is -1.

This is the mapping table of this system.

| Decimal | Binary |
| :-----: | :----: |
|    7    |  0111  |
|    6    |  0110  |
|    5    |  0101  |
|    4    |  0100  |
|    3    |  0011  |
|    2    |  0010  |
|    1    |  0001  |
|    0    |  0000  |
|   -0    |  1000  |
|   -1    |  1001  |
|   -2    |  1010  |
|   -3    |  1011  |
|   -4    |  1100  |
|   -5    |  1101  |
|   -6    |  1110  |
|   -7    |  1111  |

Probably this is more intuitive way of representing negative numbers. This system is called _signed magnitude representation_, and was actually used in early computers. Be aware **it has two zeros**.

Now let's see how `5 + (-3)` looks like in this system.

In this system, 5 and -3 are represented `0101` and `1011` respectively. If we add these two binary numbers, the result is `0101 + 1011 = 0000`. `0000` is simply 0, which is clearly wrong answer for `5 + (-3)`.

This means it can't use the addition hardware to perform this calculation, and to get the right answer it needs another hardware dedicated for subtraction.

Being able to calculate subtraction as addition is a very unique characteristic two's complement has, and it's the reason why it's so popular in computing systems.

<aside>
Indeed, <i>method of complement</i> (ones' complement, two's complement, etc) is <b>designed to implement subtraction as addition</b> of its complement number.
</aside>

[^1]: Signed numbers: Numbers that consist of both positive and negative numbers
[^2]: Ones' complement is another way to realise subtraction as addition. See [Stack Overflow — Advantage of 2's complement over 1's complement?](https://stackoverflow.com/questions/11054213/advantage-of-2s-complement-over-1s-complement) if you're interested in why two's complement is more preferred.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Power of NAND]]></title>
            <link>http://localhost:3000/blog/the-power-of-nand</link>
            <guid>http://localhost:3000/blog/the-power-of-nand</guid>
            <description><![CDATA[When I was an engineering student, I learned a logic gate NAND in one of the electronics classes. At that time I didn't fully understand why NAND is so important.

Ten years later, only after I decided to engage myself more in computer science, I finally understood why it's so: We can build very complex computing systems only with NAND on contrary to its super simple look.

I'd like to introduce here (1) what NAND is and (2) why it has such a powerful capability.]]></description>
            <content:encoded><![CDATA[
## What's NAND?

NAND is one of the most basic **logic gates**. So first I need to explain what the logic gate is.

A logic gate is a sort of a calculator that accepts one or more input and produces one or more output where both input and output values are either 0 or 1. Let's take a look at one of the simplest logic gates: AND.

<figure>
    <img src="/img/blog/the-power-of-nand/AND.svg" alt="AND gate" />
    <figcaption>AND gate</figcaption>
</figure>

As you can see it takes two inputs (a) and (b), then produce one output (out). Because each (a) and (b) can be either 0 or 1, there are 4 variations of input combination as shown in the table below. The output of an AND gate will be 1 only when both (a) and (b) are 1.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  1  |  0  |  0  |
|  0  |  1  |  0  |
|  1  |  1  |  1  |

<aside>This table representation of the relationship between input combinations and corresponding output is called <i>truth table</i>.</aside>

Let's take a look at another example: OR gate.

<figure>
    <img src="/img/blog/the-power-of-nand/OR.svg" alt="OR gate" />
    <figcaption>OR gate</figcaption>
</figure>

Again, it receives two inputs and produces one output. The output of the OR gate is 0 when both (a) and (b) are 0, otherwise 1. This is the truth table of the OR gate.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  1  |  0  |  1  |
|  0  |  1  |  1  |
|  1  |  1  |  1  |

Now let's get back to the initial question: What is NAND?

Like AND and OR gates, NAND is also a logic gate that receives two inputs and produces one output.

<figure>
    <img src="/img/blog/the-power-of-nand/NAND.svg" alt="NAND gate" />
    <figcaption>NAND gate</figcaption>
</figure>

The output of NAND gate is just an opposite of AND output: When either (a) or (b) is 1 or both of them are 0, the output is 1. More simply put, the output is 0 only when both (a) and (b) are 1.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  1  |
|  1  |  0  |  1  |
|  0  |  1  |  1  |
|  1  |  1  |  0  |

Not very complicated, right?

Now how would it be possible to build a "computer" like the one you're reading this post with with this tiny device?

## How to Build a Computer with NAND

The steps are like this:

1. build elementary logic gates such as AND, OR, and Multiplexer with NAND
1. build components needed to compose CPU such as ALU (Arithmetic Logic Unit) and RAM (Random Access Memories) with elementary logic gates
1. build a CPU with these components

An essential fact that makes these steps possible is that _any logic gates can be constructed by combining only NAND gates_. Let's take a look at an example.

<figure>
    <img src="/img/blog/the-power-of-nand/AND with NAND.svg" alt="Input (a) and (b) are connected to two different NANDs. The output of these two NANDs are connected to another NAND." />
    <figcaption>???</figcaption>
</figure>

There are three NAND gates in this combined logic gate and it still has two inputs and one output. The branching lines from (a) and (b) mean that input (a) and (b) are connected to multiple gates. Please take a moment to think how the truth table of this logic gate would look like.

This is the answer.

|  a  |  b  | out |
| :-: | :-: | :-: |
|  0  |  0  |  0  |
|  1  |  0  |  0  |
|  0  |  1  |  0  |
|  1  |  1  |  1  |

As you can see, it's identical to the AND gate; we successfully made an AND gate only with NAND gates.

What about this one?

<figure>
    <img src="/img/blog/the-power-of-nand/OR with NAND.svg" alt="Input (a) are branched into two and connected to the same NAND. Same connection for (b). The output of these two NANDs are connected to another NAND." />
    <figcaption>???</figcaption>
</figure>

As you may have guessed, it's an OR gate implemented with NAND gates.

The truth tables we've seen so far were quite simple: two inputs and one output, but it can be very complex in real life. However, it is known that no matter how complex the truth table is, it can be realised by combining only NAND gates. This characteristic of NAND is called **functional completeness**.

<aside>In fact NAND is not the only gate that has functional completeness: NOR gate (the inverse of OR gate), or the combination of AND and NOT also has this property.</aside>

Because so called CPU and any other components needed to build a computer are just a very complex version of logic gates, all the necessary components to build a computer are constructed only with NANDs.

Of course tons of arduous work needs to be done on top of this to turn it into a full-fledged computer we use today, however, they are all about how to make use of this tiny yet powerful device.

## "The Elements of Computing Systems"

Let me introduce a book which I learned this fact from at the end of the story.

In the previous post I said I'm going to work on the "CPU Experiment". However, I didn't know what exactly I should work on or how to kick off the project. After some intensive search on the internet, I came across this book titled ["The Elements of Computing Systems"](https://www.amazon.co.uk/Elements-Computing-Systems-second-Principles/dp/0262539802/).

The book was written by Dr. Noam Nisan and Dr. Shimon Schocken, and was first published in 2005 as a computer science textbook to teach their students how the black-boxed computer systems actually look inside.

The key concept of the book is "NAND to Tetris": The idea that any general-purpose computers are made in a same way that

- (1) they are built from NAND as an elementary logic gate, and
- (2) they are programmed to run any applications such as Tetris game.

And then there is a word _to_ between them meaning the book provides readers a hands-on experience of building a Tetris (general-purpose computer) starting from NAND (an elementary logic gate).

It turned out that not only is this book a perfect introduction to the low-level systems but also it was a perfect timing for me to read. Most importantly, it helped me understand what exactly I'm trying to learn.

I can't wait to see what comes out of this journey. Stay tuned…
]]></content:encoded>
        </item>
    </channel>
</rss>